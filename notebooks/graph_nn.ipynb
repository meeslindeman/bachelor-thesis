{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a84baa8-06d0-4e20-be7e-783632798c80",
   "metadata": {},
   "source": [
    "# Game\n",
    "We create a simple game in which \n",
    "- There are two agents (sender and receiver)\n",
    "- In each game, the sender is given a target graph, the receiver is given the target graph and some distractor graphs\n",
    "- The sender generates a message describing the whole target graph, and the receiver needs to point out which graph is the target (target graph vs distractor graphs)\n",
    "\n",
    "In this game, our graphs are from the `MNIST` dataset, which is for graph classification. (See [here](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.GNNBenchmarkDataset.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76807d44-7d63-4f57-a7d3-c648277c7873",
   "metadata": {},
   "source": [
    "We start with importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7753966-2880-4cee-94d6-4e1d6bd212ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import egg.core as core\n",
    "import torch.nn as nn\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "from typing import Any, List, Optional, Sequence, Union\n",
    "\n",
    "import torch.utils.data\n",
    "from torch_geometric.data import Batch, Dataset\n",
    "from torch_geometric.data.data import BaseData\n",
    "from torch_geometric.data.datapipes import DatasetAdapter\n",
    "from torch_geometric.data.on_disk_dataset import OnDiskDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da5f5ef-389d-4d58-95a8-4f40b693e769",
   "metadata": {},
   "source": [
    "# Data\n",
    "Each game has a target graph and a number of distractor graphs. In total there are `game_size` graphs. We stack `game_size` graphs as bellow: \n",
    "- the first graph is always the target (note: as long as we don't make use of position, it doesn't matter where the target graph is located in the stack)\n",
    "- the next `game_size - 1` graphs are distractors\n",
    "\n",
    "A minibatch is a list of `batch_size` graphs in which the first `game_size` graphs are for the first game, the second `game_size` graphs are for the second game, and so on. Therefore, a minibatch contains graphs for `batch_size // game_size` games. (Note: we will ignore the last `batch_size % game_size` graphs). \n",
    "\n",
    "We will make use of the mini-batching mechanism of pytorch_geometric [here](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html#mini-batches). The below is two classes for data loading. They are modification of the classes from pytorch_geometric (see the comments for the differences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6bc988e-a54c-41ac-9f04-ca26b2ad6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collater:\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_size: int,  # the number of graphs for a game\n",
    "        dataset: Union[Dataset, Sequence[BaseData], DatasetAdapter],\n",
    "        follow_batch: Optional[List[str]] = None,\n",
    "        exclude_keys: Optional[List[str]] = None,\n",
    "    ):\n",
    "        self.game_size = game_size\n",
    "        self.dataset = dataset\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "    def __call__(self, batch: List[Any]) -> Any:\n",
    "        # Check if the batch is smaller than game_size\n",
    "        if len(batch) < self.game_size:\n",
    "            return print(\"Test\")  # Or return an appropriate placeholder\n",
    "        \n",
    "        elem = batch[0]\n",
    "        if isinstance(elem, BaseData):\n",
    "            batch = batch[:((len(batch) // self.game_size) * self.game_size)]  # we throw away the last batch_size % game_size\n",
    "            batch = Batch.from_data_list(\n",
    "                batch,\n",
    "                follow_batch=self.follow_batch,\n",
    "                exclude_keys=self.exclude_keys,\n",
    "            )\n",
    "            # we return a tuple (sender_input, labels, receiver_input, aux_input)\n",
    "            # we use aux_input to store minibatch of graphs\n",
    "            return (\n",
    "                torch.zeros(len(batch) // self.game_size, 1),  # we don't need sender_input --> create a fake one\n",
    "                torch.zeros(len(batch) // self.game_size).long(),  # the target is aways the first graph among game_size graphs\n",
    "                None,  # we don't care about receiver_input\n",
    "                batch  # this is a compact data for batch_size graphs \n",
    "            )\n",
    "\n",
    "        raise TypeError(f\"DataLoader found invalid type: '{type(elem)}'\")\n",
    "\n",
    "    def collate_fn(self, batch: List[Any]) -> Any:\n",
    "        if isinstance(self.dataset, OnDiskDataset):\n",
    "            return self(self.dataset.multi_get(batch))\n",
    "        return self(batch)\n",
    "\n",
    "\n",
    "class DataLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_size: int,  # the number of graphs for a game\n",
    "        dataset: Union[Dataset, Sequence[BaseData], DatasetAdapter],\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        follow_batch: Optional[List[str]] = None,\n",
    "        exclude_keys: Optional[List[str]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.game_size = game_size\n",
    "        # Remove for PyTorch Lightning:\n",
    "        kwargs.pop('collate_fn', None)\n",
    "\n",
    "        # Save for PyTorch Lightning < 1.6:\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "        self.collator = Collater(game_size, dataset, follow_batch, exclude_keys)\n",
    "\n",
    "        if isinstance(dataset, OnDiskDataset):\n",
    "            dataset = range(len(dataset))\n",
    "\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            shuffle,\n",
    "            collate_fn=self.collator.collate_fn,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81b543-295b-4a93-b866-846dc42798be",
   "metadata": {},
   "source": [
    "# Agents \n",
    "Now we declare agent classes. Firstly, we need to build graph neural networks. Here for simplicity we use one graph-conv layer (like [here](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html#learning-methods-on-graphs)). The class computes a global graph embedding for each graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a90cf1e-613a-4642-9db5-a1afbf38c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, n_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, n_hidden)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index  # check number of graphs via: data.num_graphs\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = scatter(x, data.batch, dim=0, reduce='mean')  # size: data.num_graphs * n_hidden\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a9b709-e800-4ea6-b4f4-4b7f120b0209",
   "metadata": {},
   "source": [
    "Note the line `x = scatter(x, data.batch, dim=0, reduce='mean')` -- which is to calculate graph embeddings: the graph embedding is the mean of the node embeddings. See [here](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html#mini-batches) for why using `scatter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9452a2-c220-425e-b563-079ab65cc206",
   "metadata": {},
   "source": [
    "The two agents (sender and receiver) each has one GCN. Because we have to follow the notations of EGG, we will make use of `aux_input` to send a minibatch of graphs to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66446a3-ffd2-4351-ad24-056e73ec98aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender(nn.Module):\n",
    "    def __init__(self, num_node_features, n_hidden, game_size):\n",
    "        super().__init__()\n",
    "        self.game_size = game_size\n",
    "        self.gcn = GCN(num_node_features, n_hidden)\n",
    "        self.fc1 = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "    def forward(self, x, _aux_input):\n",
    "        # _aux_input is a minibatch of n_games x game_size graphs\n",
    "        # we don't care about x\n",
    "        data = _aux_input\n",
    "        assert data.num_graphs % self.game_size == 0\n",
    "        x = self.gcn(data)[::self.game_size]  # we just need the target graph, hence we take only the first graph of every game_size graphs\n",
    "        return self.fc1(x)  # size: n_games * n_hidden  (note: n_games = batch_size // game_size)\n",
    "\n",
    "\n",
    "class Receiver(nn.Module):\n",
    "    def __init__(self, num_node_features, n_hidden, game_size):\n",
    "        super().__init__()\n",
    "        self.game_size = game_size\n",
    "        self.gcn = GCN(num_node_features, n_hidden)\n",
    "        self.fc1 = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "    def forward(self, x, _input, _aux_input):\n",
    "        # x is the tensor of shape n_games * n_hidden -- each row is an embedding decoded from the message sent by the sender\n",
    "        cands = self.gcn(_aux_input)  # graph embeddings for all batch_size graphs; size: batch_size * n_hidden\n",
    "        cands = cands.view(cands.shape[0] // self.game_size, self.game_size, -1)  # size: n_games * game_size * n_hidden \n",
    "        dots = torch.matmul(cands, torch.unsqueeze(x, dim=-1))  # size: n_games * game_size * 1\n",
    "        print(dots.squeeze().shape)\n",
    "        return dots.squeeze()  # size: n_games * game_size: each row is a list of scores for a game (each score tells how good the corresponding candidate is) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c76cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset\n",
    "from graph.build import create_family_tree, create_data_object\n",
    "import random\n",
    "import os\n",
    "\n",
    "class FamilyGraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for generating family graph data.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory path.\n",
    "        number_of_graphs (int): Number of graphs to generate.\n",
    "        generations (int): Number of generations in each family tree.\n",
    "\n",
    "    Returns:\n",
    "        Data(x=[8, 2], edge_index=[2, 20], edge_attr=[20], labels=[8])\n",
    "    \"\"\"\n",
    "    def __init__(self, root, number_of_graphs, generations, transform=None, pre_transform=None):\n",
    "        self.number_of_graphs = number_of_graphs\n",
    "        self.generations = generations\n",
    "        super(FamilyGraphDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data = None\n",
    "        self.process()\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['family_graphs.pt']\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def generate_labels(self, num_nodes):\n",
    "        target_node_idx = random.randint(0, num_nodes - 1)\n",
    "        return target_node_idx\n",
    "    \n",
    "    def process(self):\n",
    "        if not os.path.isfile(self.processed_paths[0]):\n",
    "            self.data = []\n",
    "            for _ in range(self.number_of_graphs):\n",
    "                family_tree = create_family_tree(self.generations)\n",
    "                graph_data = create_data_object(family_tree)\n",
    "\n",
    "                # Generate random labels for each node\n",
    "                target_node_idx = self.generate_labels(graph_data.num_nodes)\n",
    "\n",
    "                # Store the labels as an attribute of the graph_data\n",
    "                graph_data.target_node_idx = target_node_idx\n",
    "\n",
    "                self.data.append(graph_data)\n",
    "\n",
    "            torch.save(self.data, self.processed_paths[0])\n",
    "        else:\n",
    "            self.data = torch.load(self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783ba02-79c6-4d88-b133-cc38a50c90bb",
   "metadata": {},
   "source": [
    "# Training \n",
    "We reuse code from [here](https://github.com/facebookresearch/EGG/blob/main/egg/zoo/basic_games/README.md#discrimination-game) from EGG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b96b58a6-d346-4cfb-81f0-66650eae0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_params(params):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # arguments concerning the training method\n",
    "    parser.add_argument(\n",
    "        \"--mode\",\n",
    "        type=str,\n",
    "        default=\"rf\",\n",
    "        help=\"Selects whether Reinforce or Gumbel-Softmax relaxation is used for training {rf, gs} (default: rf)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--temperature\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"GS temperature for the sender, only relevant in Gumbel-Softmax (gs) mode (default: 1.0)\",\n",
    "    )\n",
    "    # arguments concerning the agent architectures\n",
    "    parser.add_argument(\n",
    "        \"--sender_cell\",\n",
    "        type=str,\n",
    "        default=\"rnn\",\n",
    "        help=\"Type of the cell used for Sender {rnn, gru, lstm} (default: rnn)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--receiver_cell\",\n",
    "        type=str,\n",
    "        default=\"rnn\",\n",
    "        help=\"Type of the cell used for Receiver {rnn, gru, lstm} (default: rnn)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sender_hidden\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Size of the hidden layer of Sender (default: 10)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--receiver_hidden\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Size of the hidden layer of Receiver (default: 10)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sender_embedding\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Output dimensionality of the layer that embeds symbols produced at previous step in Sender (default: 10)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--receiver_embedding\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Output dimensionality of the layer that embeds the message symbols for Receiver (default: 10)\",\n",
    "    )\n",
    "    # arguments controlling the script output\n",
    "    parser.add_argument(\n",
    "        \"--print_validation_events\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"If this flag is passed, at the end of training the script prints the input validation data, the corresponding messages produced by the Sender, and the output probabilities produced by the Receiver (default: do not print)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--game_size\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"The number of graphs in a game (including a target and distractors) (default: 4)\",\n",
    "    )\n",
    "    args = core.init(parser, params)\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(params):\n",
    "    opts = get_params(params)\n",
    "    print(opts, flush=True)\n",
    "    game_size = opts.game_size\n",
    "\n",
    "    # we care about the communication success: the accuracy that the receiver can distinguish the target from distractors\n",
    "    def loss(\n",
    "        _sender_input,\n",
    "        _message,\n",
    "        _receiver_input,\n",
    "        receiver_output,\n",
    "        labels,\n",
    "        _aux_input,\n",
    "    ):\n",
    "        acc = (receiver_output.argmax(dim=1) == labels).detach().float()\n",
    "        loss = F.cross_entropy(receiver_output, labels, reduction=\"none\")\n",
    "        return loss, {\"acc\": acc}\n",
    "\n",
    "    dataset = FamilyGraphDataset(root='/Users/meeslindeman/Library/Mobile Documents/com~apple~CloudDocs/Thesis/Code/data', number_of_graphs=2048, generations=3)\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_loader = DataLoader(game_size=opts.game_size, dataset=train_data, batch_size=opts.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(game_size=opts.game_size, dataset=val_data, batch_size=opts.batch_size, shuffle=True)\n",
    "    \n",
    "    # we create dataset and dataloader\n",
    "    # dataname = 'MNIST'\n",
    "    # root = f'/tmp/{dataname}'\n",
    "    # train_dataset = GNNBenchmarkDataset(root=root, name=dataname, split='train')\n",
    "    # train_loader = DataLoader(game_size, train_dataset, batch_size=opts.batch_size, shuffle=True)\n",
    "    # val_dataset = GNNBenchmarkDataset(root=root, name=dataname, split='val')\n",
    "    # val_loader = DataLoader(game_size, val_dataset, batch_size=opts.batch_size, shuffle=True)\n",
    "\n",
    "    # we create the two agents\n",
    "    receiver = Receiver(dataset[0].num_node_features, n_hidden=opts.receiver_hidden, game_size=game_size)\n",
    "    sender = Sender(dataset[0].num_node_features, n_hidden=opts.sender_hidden, game_size=game_size)\n",
    "\n",
    "    sender = core.RnnSenderReinforce(\n",
    "        sender,\n",
    "        vocab_size=opts.vocab_size,\n",
    "        embed_dim=opts.sender_embedding,\n",
    "        hidden_size=opts.sender_hidden,\n",
    "        cell=opts.sender_cell,\n",
    "        max_len=opts.max_len,\n",
    "    )\n",
    "    receiver = core.RnnReceiverDeterministic(\n",
    "        receiver,\n",
    "        vocab_size=opts.vocab_size,\n",
    "        embed_dim=opts.receiver_embedding,\n",
    "        hidden_size=opts.receiver_hidden,\n",
    "        cell=opts.receiver_cell,\n",
    "    )\n",
    "    game = core.SenderReceiverRnnReinforce(\n",
    "        sender,\n",
    "        receiver,\n",
    "        loss,\n",
    "        receiver_entropy_coeff=0,\n",
    "    )\n",
    "    callbacks = []\n",
    "\n",
    "    optimizer = core.build_optimizer(game.parameters())\n",
    "    trainer = core.Trainer(\n",
    "        game=game,\n",
    "        optimizer=optimizer,\n",
    "        train_data=train_loader,\n",
    "        validation_data=val_loader,\n",
    "        callbacks=callbacks\n",
    "        + [core.ConsoleLogger(print_train_loss=True, as_json=True)],\n",
    "    )\n",
    "\n",
    "    trainer.train(n_epochs=opts.n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f3ff9-21f0-4142-b28d-ec3cf0cea935",
   "metadata": {},
   "source": [
    "Before running the training, we need to modify the EGG's code a bit. Open `[your_dir]/site-packages/egg/core/interaction.py`, change line 209 to `aux_input[k] = None`. \n",
    "\n",
    "Let's run the game. Notice that the accuracy on the val set increases. (You should keep in mind that the accuracy is *not* for graph classification. Instead it is for how well the receiver can distinguish the target from distractors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5c7ee11-706d-4a68-a814-0f983d496b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(mode='rf', temperature=1.0, sender_cell='rnn', receiver_cell='rnn', sender_hidden=10, receiver_hidden=10, sender_embedding=10, receiver_embedding=10, print_validation_events=False, game_size=4, random_seed=821793608, checkpoint_dir=None, preemptable=False, checkpoint_freq=0, validation_freq=1, n_epochs=1, load_from_checkpoint=None, no_cuda=True, batch_size=32, optimizer='adam', lr=0.01, update_freq=1, vocab_size=10, max_len=1, tensorboard=False, tensorboard_dir='runs/', distributed_port=18363, fp16=False, cuda=False, device=device(type='cpu'), distributed_context=DistributedContext(is_distributed=False, rank=0, local_rank=0, world_size=1, mode='none'))\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--n_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 144\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    134\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mbuild_optimizer(game\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    135\u001b[0m trainer \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m    136\u001b[0m     game\u001b[38;5;241m=\u001b[39mgame,\n\u001b[1;32m    137\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;241m+\u001b[39m [core\u001b[38;5;241m.\u001b[39mConsoleLogger(print_train_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, as_json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)],\n\u001b[1;32m    142\u001b[0m )\n\u001b[0;32m--> 144\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/trainers.py:273\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    271\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 273\u001b[0m train_loss, train_interaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    276\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_end(train_loss, train_interaction, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/trainers.py:216\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m context \u001b[38;5;241m=\u001b[39m autocast() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;28;01melse\u001b[39;00m nullcontext()\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m--> 216\u001b[0m     optimized_loss, interaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_freq \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;66;03m# throughout EGG, we minimize _mean_ loss, not sum\u001b[39;00m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;66;03m# hence, we need to account for that when aggregating grads\u001b[39;00m\n\u001b[1;32m    221\u001b[0m         optimized_loss \u001b[38;5;241m=\u001b[39m optimized_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_freq\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/reinforce_wrappers.py:504\u001b[0m, in \u001b[0;36mSenderReceiverRnnReinforce.forward\u001b[0;34m(self, sender_input, labels, receiver_input, aux_input)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sender_input, labels, receiver_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, aux_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmechanics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceiver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43msender_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreceiver_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43maux_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/reinforce_wrappers.py:568\u001b[0m, in \u001b[0;36mCommunicationRnnReinforce.forward\u001b[0;34m(self, sender, receiver, loss, sender_input, labels, receiver_input, aux_input)\u001b[0m\n\u001b[1;32m    563\u001b[0m message_length \u001b[38;5;241m=\u001b[39m find_lengths(message)\n\u001b[1;32m    564\u001b[0m receiver_output, log_prob_r, entropy_r \u001b[38;5;241m=\u001b[39m receiver(\n\u001b[1;32m    565\u001b[0m     message, receiver_input, aux_input, message_length\n\u001b[1;32m    566\u001b[0m )\n\u001b[0;32m--> 568\u001b[0m loss, aux_info \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43msender_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreceiver_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreceiver_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_input\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# the entropy of the outputs of S before and including the eos symbol - as we don't care about what's after\u001b[39;00m\n\u001b[1;32m    573\u001b[0m effective_entropy_s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(entropy_r)\n",
      "Cell \u001b[0;32mIn[16], line 87\u001b[0m, in \u001b[0;36mmain.<locals>.loss\u001b[0;34m(_sender_input, _message, _receiver_input, receiver_output, labels, _aux_input)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\n\u001b[1;32m     80\u001b[0m     _sender_input,\n\u001b[1;32m     81\u001b[0m     _message,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     _aux_input,\n\u001b[1;32m     86\u001b[0m ):\n\u001b[0;32m---> 87\u001b[0m     acc \u001b[38;5;241m=\u001b[39m (\u001b[43mreceiver_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     88\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(receiver_output, labels, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m: acc}\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "main([\"--n_epochs\", \"1\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
