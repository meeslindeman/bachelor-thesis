{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import egg.core as core\n",
    "from options import Options\n",
    "opts = Options()\n",
    "\n",
    "from typing import Any, List, Optional, Sequence, Union\n",
    "\n",
    "import torch.utils.data\n",
    "from torch_geometric.data import Batch, Dataset, Data\n",
    "from torch_geometric.data.data import BaseData\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data.datapipes import DatasetAdapter\n",
    "from torch_geometric.data.on_disk_dataset import OnDiskDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from torch_geometric.data import Dataset\n",
    "from graph.build import create_family_tree, create_data_object\n",
    "\n",
    "class FamilyGraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for generating family graph data.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory path.\n",
    "        number_of_graphs (int): Number of graphs to generate.\n",
    "        generations (int): Number of generations in each family tree.\n",
    "\n",
    "    Returns:\n",
    "        Data(x=[8, 2], edge_index=[2, 20], edge_attr=[20], labels=[8])\n",
    "    \"\"\"\n",
    "    def __init__(self, root, number_of_graphs, generations, transform=None, pre_transform=None):\n",
    "        self.number_of_graphs = number_of_graphs\n",
    "        self.generations = generations\n",
    "        super(FamilyGraphDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data = None\n",
    "        self.process()\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['family_graphs.pt']\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def generate_labels(self, num_nodes):\n",
    "        target_node_idx = random.randint(0, num_nodes - 1)\n",
    "        return target_node_idx\n",
    "    \n",
    "    def process(self):\n",
    "        if not os.path.isfile(self.processed_paths[0]):\n",
    "            self.data = []\n",
    "            for _ in range(self.number_of_graphs):\n",
    "                family_tree = create_family_tree(self.generations)\n",
    "                graph_data = create_data_object(family_tree)\n",
    "\n",
    "                # Generate random labels for each node\n",
    "                target_node_idx = self.generate_labels(graph_data.num_nodes)\n",
    "\n",
    "                # Store the labels as an attribute of the graph_data\n",
    "                graph_data.target_node_idx = target_node_idx\n",
    "\n",
    "                self.data.append(graph_data)\n",
    "\n",
    "            torch.save(self.data, self.processed_paths[0])\n",
    "        else:\n",
    "            self.data = torch.load(self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[4, 4], edge_index=[2, 8], edge_attr=[8], target_node_idx=2, root_idx=3)\n",
      "Data(x=[4, 4], edge_index=[2, 8], edge_attr=[8], target_node_idx=2, root_idx=0)\n"
     ]
    }
   ],
   "source": [
    "dataset = FamilyGraphDataset(root='/Users/meeslindeman/Library/Mobile Documents/com~apple~CloudDocs/Thesis/Code/data', number_of_graphs=10000, generations=3)\n",
    "print(dataset[0])\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import scatter\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, embedding_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(num_node_features, embedding_size, edge_dim=1, heads=1, concat=True)\n",
    "        self.conv2 = GATv2Conv(-1, embedding_size, edge_dim=1, heads=1, concat=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        h, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        h = self.conv1(x=h, edge_index=edge_index, edge_attr=edge_attr)     \n",
    "        h = F.relu(h)\n",
    "\n",
    "        h = self.conv2(x=h, edge_index=edge_index, edge_attr=edge_attr)     \n",
    "        h = F.relu(h) \n",
    "\n",
    "        x = F.dropout(h, training=self.training)\n",
    "        x = scatter(x, data.batch, dim=0, reduce='sum')  # size: data.num_graphs * n_hidden\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender(nn.Module):\n",
    "    def __init__(self, num_node_features, n_hidden, game_size, temperature):\n",
    "        super(Sender, self).__init__()\n",
    "        self.temp = temperature\n",
    "        self.game_size = game_size\n",
    "        self.gcn = GAT(num_node_features, n_hidden)\n",
    "\n",
    "        self.fc1 = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "    def forward(self, x, _aux_input):\n",
    "        data = _aux_input\n",
    "        assert data.num_graphs % self.game_size == 0\n",
    "        x = self.gcn(data)[::self.game_size]  # we just need the target graph, hence we take only the first graph of every game_size graphs\n",
    "        x = self.fc1(x)\n",
    "        return x  # size: n_games * n_hidden  (note: n_games = batch_size // game_size)\n",
    "\n",
    "class Receiver(nn.Module):\n",
    "    def __init__(self, num_node_features, n_hidden, game_size):\n",
    "        super().__init__()\n",
    "        self.game_size = game_size\n",
    "        self.gcn = GAT(num_node_features, n_hidden)\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "    def forward(self, x, _input, _aux_input):\n",
    "        # x is the tensor of shape n_games * n_hidden -- each row is an embedding decoded from the message sent by the sender\n",
    "        cands = self.gcn(_aux_input)  # graph embeddings for all batch_size graphs; size: batch_size * n_hidden\n",
    "        cands = cands.view(cands.shape[0] // self.game_size, self.game_size, -1)  # size: n_games * game_size * n_hidden \n",
    "        dots = torch.matmul(cands, torch.unsqueeze(x, dim=-1))  # size: n_games * game_size * 1\n",
    "        return dots.squeeze()  # size: n_games * game_size: each row is a list of scores for a game (each score tells how good the corresponding candidate is) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collater:\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_size: int,  # the number of graphs for a game\n",
    "        dataset: Union[Dataset, Sequence[BaseData], DatasetAdapter],\n",
    "        follow_batch: Optional[List[str]] = None,\n",
    "        exclude_keys: Optional[List[str]] = None,\n",
    "    ):\n",
    "        self.game_size = game_size\n",
    "        self.dataset = dataset\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "    def __call__(self, batch: List[Any]) -> Any:\n",
    "        elem = batch[0]\n",
    "        if isinstance(elem, BaseData):\n",
    "            batch = batch[:((len(batch) // self.game_size) * self.game_size)]  # we throw away the last batch_size % game_size\n",
    "            batch = Batch.from_data_list(\n",
    "                batch,\n",
    "                follow_batch=self.follow_batch,\n",
    "                exclude_keys=self.exclude_keys,\n",
    "            )\n",
    "            # we return a tuple (sender_input, labels, receiver_input, aux_input)\n",
    "            # we use aux_input to store minibatch of graphs\n",
    "            return (\n",
    "                torch.zeros(len(batch) // self.game_size, 1),  # we don't need sender_input --> create a fake one\n",
    "                torch.zeros(len(batch) // self.game_size).long(),  # the target is aways the first graph among game_size graphs\n",
    "                None,  # we don't care about receiver_input\n",
    "                batch  # this is a compact data for batch_size graphs \n",
    "            )\n",
    "\n",
    "        raise TypeError(f\"DataLoader found invalid type: '{type(elem)}'\")\n",
    "\n",
    "    def collate_fn(self, batch: List[Any]) -> Any:\n",
    "        if isinstance(self.dataset, OnDiskDataset):\n",
    "            return self(self.dataset.multi_get(batch))\n",
    "        return self(batch)\n",
    "\n",
    "\n",
    "class DataLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_size: int,  # the number of graphs for a game\n",
    "        dataset: Union[Dataset, Sequence[BaseData], DatasetAdapter],\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        follow_batch: Optional[List[str]] = None,\n",
    "        exclude_keys: Optional[List[str]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.game_size = game_size\n",
    "        # Remove for PyTorch Lightning:\n",
    "        kwargs.pop('collate_fn', None)\n",
    "\n",
    "        # Save for PyTorch Lightning < 1.6:\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "        self.collator = Collater(game_size, dataset, follow_batch, exclude_keys)\n",
    "\n",
    "        if isinstance(dataset, OnDiskDataset):\n",
    "            dataset = range(len(dataset))\n",
    "\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            shuffle,\n",
    "            collate_fn=self.collator.collate_fn,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 80\n",
      "Validation set length: 20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the lengths of the training and validation sets\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "\n",
    "train_loader = DataLoader(game_size=4, dataset=train_data, batch_size=opts.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(game_size=4, dataset=val_data, batch_size=opts.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(\n",
    "    _sender_input,\n",
    "    _message,\n",
    "    _receiver_input,\n",
    "    receiver_output,\n",
    "    labels,\n",
    "    _aux_input,\n",
    "):\n",
    "    acc = (receiver_output.argmax(dim=1) == labels).detach().float()\n",
    "    loss = F.cross_entropy(receiver_output, labels, reduction=\"none\")\n",
    "    return loss, {\"acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create the two agents\n",
    "receiver = Receiver(dataset[0].num_node_features, n_hidden=opts.hidden_size, game_size=4)\n",
    "sender = Sender(dataset[0].num_node_features, n_hidden=opts.hidden_size, game_size=4, temperature=opts.gs_tau)\n",
    "\n",
    "sender_w = core.RnnSenderReinforce(\n",
    "    sender,\n",
    "    vocab_size=opts.vocab_size,\n",
    "    embed_dim=opts.embedding_size,\n",
    "    hidden_size=opts.hidden_size,\n",
    "    cell=opts.sender_cell,\n",
    "    max_len=opts.max_len\n",
    ")\n",
    "receiver_w = core.RnnReceiverDeterministic(\n",
    "    receiver,\n",
    "    vocab_size=opts.vocab_size,\n",
    "    embed_dim=opts.embedding_size,\n",
    "    hidden_size=opts.hidden_size,\n",
    "    cell=opts.sender_cell\n",
    ")\n",
    "\n",
    "game = core.SenderReceiverRnnReinforce(sender_w, receiver_w, loss, receiver_entropy_coeff=0)\n",
    "\n",
    "opts = core.init(params=['--random_seed=7', \n",
    "                         '--lr=1e-2',   \n",
    "                         f'--batch_size=16',\n",
    "                         '--optimizer=adam'])\n",
    "\n",
    "optimizer = torch.optim.Adam(game.parameters())\n",
    "\n",
    "trainer = core.Trainer(\n",
    "    game=game, \n",
    "    optimizer=optimizer, \n",
    "    train_data=train_loader,\n",
    "    validation_data=val_loader, \n",
    "    callbacks=[core.ConsoleLogger(as_json=True, print_train_loss=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/trainers.py:273\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    271\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 273\u001b[0m train_loss, train_interaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    276\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_end(train_loss, train_interaction, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/trainers.py:209\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 209\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m, in \u001b[0;36mCollater.collate_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, OnDiskDataset):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmulti_get(batch))\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m     17\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch[:((\u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_size) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_size)]  \u001b[38;5;66;03m# we throw away the last batch_size % game_size\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# we return a tuple (sender_input, labels, receiver_input, aux_input)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# we use aux_input to store minibatch of graphs\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_size, \u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# we don't need sender_input --> create a fake one\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_size)\u001b[38;5;241m.\u001b[39mlong(),  \u001b[38;5;66;03m# the target is aways the first graph among game_size graphs\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# we don't care about receiver_input\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         batch  \u001b[38;5;66;03m# this is a compact data for batch_size graphs \u001b[39;00m\n\u001b[1;32m     30\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch_geometric/data/batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_list: List[BaseData],\n\u001b[1;32m     83\u001b[0m                    follow_batch: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m                    exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m collate(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m     95\u001b[0m         data_list\u001b[38;5;241m=\u001b[39mdata_list,\n\u001b[1;32m     96\u001b[0m         increment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m---> 97\u001b[0m         add_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, Batch),\n\u001b[1;32m     98\u001b[0m         follow_batch\u001b[38;5;241m=\u001b[39mfollow_batch,\n\u001b[1;32m     99\u001b[0m         exclude_keys\u001b[38;5;241m=\u001b[39mexclude_keys,\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    102\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)\n\u001b[1;32m    103\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
