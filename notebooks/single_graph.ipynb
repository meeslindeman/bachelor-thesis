{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import egg.core as core\n",
    "from options import Options\n",
    "opts = Options()\n",
    "\n",
    "from typing import Any, List, Optional, Sequence, Union\n",
    "\n",
    "import torch.utils.data\n",
    "from torch_geometric.data import Batch, Dataset, Data\n",
    "from torch_geometric.data.data import BaseData\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data.datapipes import DatasetAdapter\n",
    "from torch_geometric.data.on_disk_dataset import OnDiskDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from torch_geometric.data import Dataset\n",
    "from graph.build import create_family_tree, create_data_object\n",
    "\n",
    "class FamilyGraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for generating family graph data.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory path.\n",
    "        number_of_graphs (int): Number of graphs to generate.\n",
    "        generations (int): Number of generations in each family tree.\n",
    "\n",
    "    Returns:\n",
    "        Data(x=[8, 2], edge_index=[2, 20], edge_attr=[20], labels=[8])\n",
    "    \"\"\"\n",
    "    def __init__(self, root, number_of_graphs, generations, transform=None, pre_transform=None):\n",
    "        self.number_of_graphs = number_of_graphs\n",
    "        self.generations = generations\n",
    "        super(FamilyGraphDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data = None\n",
    "        self.process()\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['family_graphs.pt']\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def generate_labels(self, num_nodes):\n",
    "        target_node_idx = random.randint(0, num_nodes - 1)\n",
    "        return target_node_idx\n",
    "    \n",
    "    def process(self):\n",
    "        if not os.path.isfile(self.processed_paths[0]):\n",
    "            self.data = []\n",
    "            for _ in range(self.number_of_graphs):\n",
    "                family_tree = create_family_tree(self.generations)\n",
    "                graph_data = create_data_object(family_tree)\n",
    "\n",
    "                # Generate random labels for each node\n",
    "                target_node_idx = self.generate_labels(graph_data.num_nodes)\n",
    "\n",
    "                # Store the labels as an attribute of the graph_data\n",
    "                graph_data.target_node_idx = target_node_idx\n",
    "\n",
    "                self.data.append(graph_data)\n",
    "\n",
    "            torch.save(self.data, self.processed_paths[0])\n",
    "        else:\n",
    "            self.data = torch.load(self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[18, 2], edge_index=[2, 42], edge_attr=[42], target_node_idx=3)\n",
      "Data(x=[5, 2], edge_index=[2, 12], edge_attr=[12], target_node_idx=2)\n"
     ]
    }
   ],
   "source": [
    "dataset = FamilyGraphDataset(root='/Users/meeslindeman/Library/Mobile Documents/com~apple~CloudDocs/Thesis/Code/data', number_of_graphs=10000, generations=3)\n",
    "print(dataset[0])\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import scatter\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, embedding_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(num_node_features, embedding_size, edge_dim=1, heads=1, concat=True)\n",
    "        self.conv2 = GATv2Conv(-1, embedding_size, edge_dim=1, heads=1, concat=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        h, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        h = self.conv1(x=h, edge_index=edge_index, edge_attr=edge_attr)     \n",
    "        h = F.relu(h)\n",
    "\n",
    "        h = self.conv2(x=h, edge_index=edge_index, edge_attr=edge_attr)     \n",
    "        h = F.relu(h) \n",
    "\n",
    "        x = F.dropout(h, training=self.training)\n",
    "        x = scatter(x, data.batch, dim=0, reduce='sum')  # size: data.num_graphs * n_hidden\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender(nn.Module):\n",
    "    def __init__(self, num_node_features, n_hidden, game_size, temperature):\n",
    "        super(Sender, self).__init__()\n",
    "        self.temp = temperature\n",
    "        self.game_size = game_size\n",
    "        self.gcn = GAT(num_node_features, n_hidden)\n",
    "\n",
    "        self.fc1 = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "    def forward(self, x, _aux_input):\n",
    "        data = _aux_input\n",
    "        assert data.num_graphs % self.game_size == 0\n",
    "        x = self.gcn(data)[::self.game_size]  # we just need the target graph, hence we take only the first graph of every game_size graphs\n",
    "        x = self.fc1(x)\n",
    "        return x  # size: n_games * n_hidden  (note: n_games = batch_size // game_size)\n",
    "\n",
    "class Receiver(nn.Module):\n",
    "    def __init__(self, num_node_features, n_hidden, game_size):\n",
    "        super().__init__()\n",
    "        self.game_size = game_size\n",
    "        self.gcn = GAT(num_node_features, n_hidden)\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "    def forward(self, x, _input, _aux_input):\n",
    "        # x is the tensor of shape n_games * n_hidden -- each row is an embedding decoded from the message sent by the sender\n",
    "        cands = self.gcn(_aux_input)  # graph embeddings for all batch_size graphs; size: batch_size * n_hidden\n",
    "        cands = cands.view(cands.shape[0] // self.game_size, self.game_size, -1)  # size: n_games * game_size * n_hidden \n",
    "        dots = torch.matmul(cands, torch.unsqueeze(x, dim=-1))  # size: n_games * game_size * 1\n",
    "        return dots.squeeze()  # size: n_games * game_size: each row is a list of scores for a game (each score tells how good the corresponding candidate is) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collater:\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_size: int,  # the number of graphs for a game\n",
    "        dataset: Union[Dataset, Sequence[BaseData], DatasetAdapter],\n",
    "        follow_batch: Optional[List[str]] = None,\n",
    "        exclude_keys: Optional[List[str]] = None,\n",
    "    ):\n",
    "        self.game_size = game_size\n",
    "        self.dataset = dataset\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "    def __call__(self, batch: List[Any]) -> Any:\n",
    "        elem = batch[0]\n",
    "        if isinstance(elem, BaseData):\n",
    "            batch = batch[:((len(batch) // self.game_size) * self.game_size)]  # we throw away the last batch_size % game_size\n",
    "            batch = Batch.from_data_list(\n",
    "                batch,\n",
    "                follow_batch=self.follow_batch,\n",
    "                exclude_keys=self.exclude_keys,\n",
    "            )\n",
    "            # we return a tuple (sender_input, labels, receiver_input, aux_input)\n",
    "            # we use aux_input to store minibatch of graphs\n",
    "            return (\n",
    "                torch.zeros(len(batch) // self.game_size, 1),  # we don't need sender_input --> create a fake one\n",
    "                torch.zeros(len(batch) // self.game_size).long(),  # the target is aways the first graph among game_size graphs\n",
    "                None,  # we don't care about receiver_input\n",
    "                batch  # this is a compact data for batch_size graphs \n",
    "            )\n",
    "\n",
    "        raise TypeError(f\"DataLoader found invalid type: '{type(elem)}'\")\n",
    "\n",
    "    def collate_fn(self, batch: List[Any]) -> Any:\n",
    "        if isinstance(self.dataset, OnDiskDataset):\n",
    "            return self(self.dataset.multi_get(batch))\n",
    "        return self(batch)\n",
    "\n",
    "\n",
    "class DataLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_size: int,  # the number of graphs for a game\n",
    "        dataset: Union[Dataset, Sequence[BaseData], DatasetAdapter],\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        follow_batch: Optional[List[str]] = None,\n",
    "        exclude_keys: Optional[List[str]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.game_size = game_size\n",
    "        # Remove for PyTorch Lightning:\n",
    "        kwargs.pop('collate_fn', None)\n",
    "\n",
    "        # Save for PyTorch Lightning < 1.6:\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "        self.collator = Collater(game_size, dataset, follow_batch, exclude_keys)\n",
    "\n",
    "        if isinstance(dataset, OnDiskDataset):\n",
    "            dataset = range(len(dataset))\n",
    "\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            shuffle,\n",
    "            collate_fn=self.collator.collate_fn,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 8000\n",
      "Validation set length: 2000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the lengths of the training and validation sets\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "\n",
    "train_loader = DataLoader(game_size=opts.game_size, dataset=train_data, batch_size=opts.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(game_size=opts.game_size, dataset=val_data, batch_size=opts.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(\n",
    "    _sender_input,\n",
    "    _message,\n",
    "    _receiver_input,\n",
    "    receiver_output,\n",
    "    labels,\n",
    "    _aux_input,\n",
    "):\n",
    "    acc = (receiver_output.argmax(dim=1) == labels).detach().float()\n",
    "    loss = F.cross_entropy(receiver_output, labels, reduction=\"none\")\n",
    "    return loss, {\"acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create the two agents\n",
    "receiver = Receiver(dataset[0].num_node_features, n_hidden=opts.hidden_size, game_size=opts.game_size)\n",
    "sender = Sender(dataset[0].num_node_features, n_hidden=opts.hidden_size, game_size=opts.game_size, temperature=opts.gs_tau)\n",
    "\n",
    "sender_w = core.RnnSenderReinforce(\n",
    "    sender,\n",
    "    vocab_size=opts.vocab_size,\n",
    "    embed_dim=opts.embedding_size,\n",
    "    hidden_size=opts.hidden_size,\n",
    "    cell=opts.sender_cell,\n",
    "    max_len=opts.max_len\n",
    ")\n",
    "receiver_w = core.RnnReceiverDeterministic(\n",
    "    receiver,\n",
    "    vocab_size=opts.vocab_size,\n",
    "    embed_dim=opts.embedding_size,\n",
    "    hidden_size=opts.hidden_size,\n",
    "    cell=opts.sender_cell\n",
    ")\n",
    "\n",
    "game = core.SenderReceiverRnnReinforce(sender_w, receiver_w, loss, receiver_entropy_coeff=0)\n",
    "\n",
    "opts = core.init(params=['--random_seed=7', \n",
    "                         '--lr=1e-2',   \n",
    "                         f'--batch_size={opts.batch_size}',\n",
    "                         '--optimizer=adam'])\n",
    "\n",
    "optimizer = torch.optim.Adam(game.parameters())\n",
    "\n",
    "trainer = core.Trainer(\n",
    "    game=game, \n",
    "    optimizer=optimizer, \n",
    "    train_data=train_loader,\n",
    "    validation_data=val_loader, \n",
    "    callbacks=[core.ConsoleLogger(as_json=True, print_train_loss=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 250.5278778076172, \"acc\": 0.04749999940395355, \"sender_entropy\": 2.1901023387908936, \"receiver_entropy\": 0.0, \"length\": 1.9850000143051147, \"mode\": \"train\", \"epoch\": 1}\n",
      "{\"loss\": 204.7561492919922, \"acc\": 0.07999999821186066, \"sender_entropy\": 2.150277614593506, \"receiver_entropy\": 0.0, \"length\": 2.0, \"mode\": \"test\", \"epoch\": 1}\n",
      "{\"loss\": 185.22689819335938, \"acc\": 0.08250000327825546, \"sender_entropy\": 2.143399715423584, \"receiver_entropy\": 0.0, \"length\": 1.9774999618530273, \"mode\": \"train\", \"epoch\": 2}\n",
      "{\"loss\": 100.58894348144531, \"acc\": 0.019999999552965164, \"sender_entropy\": 2.1419339179992676, \"receiver_entropy\": 0.0, \"length\": 2.0, \"mode\": \"test\", \"epoch\": 2}\n",
      "{\"loss\": 122.75272369384766, \"acc\": 0.17749999463558197, \"sender_entropy\": 2.162497043609619, \"receiver_entropy\": 0.0, \"length\": 1.9824999570846558, \"mode\": \"train\", \"epoch\": 3}\n",
      "{\"loss\": 76.38700866699219, \"acc\": 0.05000000074505806, \"sender_entropy\": 2.159968137741089, \"receiver_entropy\": 0.0, \"length\": 2.0, \"mode\": \"test\", \"epoch\": 3}\n",
      "{\"loss\": 87.86172485351562, \"acc\": 0.5249999761581421, \"sender_entropy\": 2.156125545501709, \"receiver_entropy\": 0.0, \"length\": 1.9700000286102295, \"mode\": \"train\", \"epoch\": 4}\n",
      "{\"loss\": 58.330116271972656, \"acc\": 0.07999999821186066, \"sender_entropy\": 2.1528682708740234, \"receiver_entropy\": 0.0, \"length\": 2.0, \"mode\": \"test\", \"epoch\": 4}\n",
      "{\"loss\": 68.72555541992188, \"acc\": 0.48750001192092896, \"sender_entropy\": 2.1510210037231445, \"receiver_entropy\": 0.0, \"length\": 1.9824999570846558, \"mode\": \"train\", \"epoch\": 5}\n",
      "{\"loss\": 46.06687927246094, \"acc\": 0.10999999940395355, \"sender_entropy\": 2.1521189212799072, \"receiver_entropy\": 0.0, \"length\": 2.0, \"mode\": \"test\", \"epoch\": 5}\n",
      "{\"loss\": 55.143821716308594, \"acc\": 0.9325000047683716, \"sender_entropy\": 2.145073175430298, \"receiver_entropy\": 0.0, \"length\": 1.9774999618530273, \"mode\": \"train\", \"epoch\": 6}\n",
      "{\"loss\": 39.101112365722656, \"acc\": 0.8700000047683716, \"sender_entropy\": 2.133157968521118, \"receiver_entropy\": 0.0, \"length\": 2.0, \"mode\": \"test\", \"epoch\": 6}\n",
      "{\"loss\": 47.17817306518555, \"acc\": 0.9399999976158142, \"sender_entropy\": 2.126168727874756, \"receiver_entropy\": 0.0, \"length\": 1.9824999570846558, \"mode\": \"train\", \"epoch\": 7}\n",
      "{\"loss\": 31.706153869628906, \"acc\": 0.8999999761581421, \"sender_entropy\": 2.117661952972412, \"receiver_entropy\": 0.0, \"length\": 2.0, \"mode\": \"test\", \"epoch\": 7}\n",
      "{\"loss\": 42.41022872924805, \"acc\": 0.9574999809265137, \"sender_entropy\": 2.1218152046203613, \"receiver_entropy\": 0.0, \"length\": 1.9924999475479126, \"mode\": \"train\", \"epoch\": 8}\n",
      "{\"loss\": 28.171092987060547, \"acc\": 0.9800000190734863, \"sender_entropy\": 2.1204299926757812, \"receiver_entropy\": 0.0, \"length\": 2.0, \"mode\": \"test\", \"epoch\": 8}\n",
      "{\"loss\": 37.18505859375, \"acc\": 0.9524999856948853, \"sender_entropy\": 2.1227469444274902, \"receiver_entropy\": 0.0, \"length\": 1.9900000095367432, \"mode\": \"train\", \"epoch\": 9}\n",
      "{\"loss\": 25.30707550048828, \"acc\": 0.9900000095367432, \"sender_entropy\": 2.1176352500915527, \"receiver_entropy\": 0.0, \"length\": 2.0, \"mode\": \"test\", \"epoch\": 9}\n",
      "{\"loss\": 33.45045471191406, \"acc\": 0.9750000238418579, \"sender_entropy\": 2.120316505432129, \"receiver_entropy\": 0.0, \"length\": 1.9900000095367432, \"mode\": \"train\", \"epoch\": 10}\n",
      "{\"loss\": 23.13365936279297, \"acc\": 1.0, \"sender_entropy\": 2.1252427101135254, \"receiver_entropy\": 0.0, \"length\": 2.0, \"mode\": \"test\", \"epoch\": 10}\n"
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
