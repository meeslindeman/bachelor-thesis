{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import egg.core as core\n",
    "\n",
    "from typing import Any, List, Optional, Sequence, Union\n",
    "\n",
    "import torch.utils.data\n",
    "from torch_geometric.data import Batch, Dataset, Data\n",
    "from torch_geometric.data.data import BaseData\n",
    "from torch_geometric.data.datapipes import DatasetAdapter\n",
    "from torch_geometric.data.on_disk_dataset import OnDiskDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Options:\n",
    "    # Agents\n",
    "    embedding_size: int = 10 # Default: 50\n",
    "    heads: int = 2 # Default: 1\n",
    "    hidden_size: int = 20 # Default: 20\n",
    "    sender_cell: str = 'gru' # 'rnn', 'gru', 'lstm'\n",
    "    max_len: int = 4 # Default: 1\n",
    "    gs_tau: int = 1.0 # Default: 1.0\n",
    "\n",
    "    # Training\n",
    "    n_epochs: int = 5\n",
    "    agents: str = 'dual' # 'dual', 'transform', 'gat'\n",
    "    vocab_size: int = 100 # Default: 100\n",
    "    batch_size: int = 1 \n",
    "\n",
    "opts = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class FamilyMember:\n",
    "    \"\"\"\n",
    "    Represents a family member with gender, age, spouse, and children.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gender, age):\n",
    "        self.gender = gender\n",
    "        self.age = age\n",
    "        self.spouse = None\n",
    "        self.children = []\n",
    "        self.height = random.randint(150, 200) # in cm\n",
    "        self.hair_color = random.choice(['black', 'brown', 'blonde', 'red'])\n",
    "\n",
    "    def create_spouse(self):\n",
    "        \"\"\"\n",
    "        Creates a spouse for the family member based on their gender and age.\n",
    "        Returns the created spouse.\n",
    "        \"\"\"\n",
    "        spouse_gender = 'f' if self.gender == 'm' else 'm'\n",
    "        spouse_age = random.randint(max(18, self.age - 5), min(self.age + 5, 100))\n",
    "        spouse = FamilyMember(spouse_gender, spouse_age)\n",
    "        self.spouse = spouse\n",
    "        spouse.spouse = self\n",
    "        return spouse\n",
    "\n",
    "    def create_children(self, max_children=4):\n",
    "        \"\"\"\n",
    "        Creates children for the family member and their spouse.\n",
    "        \"\"\"\n",
    "        children_count = random.randint(1, max_children) # Set this to a integer for graphs of same size\n",
    "        youngest_parent_age = min(self.age, self.spouse.age)\n",
    "        for _ in range(children_count):\n",
    "            child_gender = random.choice(['m', 'f'])\n",
    "            max_child_age = max(0, youngest_parent_age - 20)\n",
    "            min_child_age = max(0, youngest_parent_age - 30)\n",
    "            child_age = random.randint(min_child_age, max_child_age)\n",
    "            child = FamilyMember(child_gender, child_age)\n",
    "            self.children.append(child)\n",
    "            self.spouse.children.append(child)\n",
    "\n",
    "def create_family_tree(generations):\n",
    "    age_range = (80,100)\n",
    "    root_age = random.randint(*age_range)\n",
    "    root_member = FamilyMember('m', root_age)\n",
    "\n",
    "    spouse = root_member.create_spouse()\n",
    "    current_generation = [(root_member, spouse)]\n",
    "    \n",
    "    all_members = {0: root_member, 1: spouse}\n",
    "    next_index = 2  # Start indexing from 2 as 0 and 1 are already used\n",
    "\n",
    "    for gen in range(1, generations):\n",
    "        next_generation = []\n",
    "        for parent1, parent2 in current_generation:\n",
    "            parent1.create_children()\n",
    "            for child in parent1.children:\n",
    "                all_members[next_index] = child\n",
    "                next_index += 1\n",
    "                if gen < generations - 1:\n",
    "                    spouse = child.create_spouse()\n",
    "                    all_members[next_index] = spouse\n",
    "                    next_generation.append((child, spouse))\n",
    "                    next_index += 1\n",
    "        current_generation = next_generation\n",
    "\n",
    "    return all_members\n",
    "\n",
    "def create_data_object(all_members):\n",
    "    # Convert genders to a binary representation and collect node features\n",
    "    gender_to_binary = {'m': 0, 'f': 1}\n",
    "    color_to_binary = {'black': 0, 'brown': 1, 'blonde': 2, 'red': 3}\n",
    "    x = [[gender_to_binary[member.gender], member.age, color_to_binary[member.hair_color], member.height] for index, member in all_members.items()]\n",
    "\n",
    "    # Prepare edge_index and edge_attr\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "\n",
    "    for index, member in all_members.items():\n",
    "        if member.spouse:\n",
    "            spouse_index = list(all_members.keys())[list(all_members.values()).index(member.spouse)]\n",
    "            # Add edges for spouses in both directions with the 'married' attribute\n",
    "            edge_index.append([index, spouse_index])\n",
    "            edge_index.append([spouse_index, index])\n",
    "            edge_attr.extend([0, 0])  # 0 for 'married'\n",
    "\n",
    "        for child in member.children:\n",
    "            child_index = list(all_members.keys())[list(all_members.values()).index(child)]\n",
    "            # Add edges from children to this member with the 'childOf' attribute\n",
    "            edge_index.append([child_index, index])\n",
    "            edge_attr.append(1)  # 1 for 'childOf'\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "    # Create the data object\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "class FamilyGraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for generating family graph data.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory path.\n",
    "        number_of_graphs (int): Number of graphs to generate.\n",
    "        generations (int): Number of generations in each family tree.\n",
    "\n",
    "    Returns:\n",
    "        Data(x=[8, 2], edge_index=[2, 20], edge_attr=[20], labels=[8])\n",
    "    \"\"\"\n",
    "    def __init__(self, root, number_of_graphs, generations, transform=None, pre_transform=None):\n",
    "        self.number_of_graphs = number_of_graphs\n",
    "        self.generations = generations\n",
    "        super(FamilyGraphDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data = None\n",
    "        self.process()\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['family_graphs.pt']\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def generate_labels(self, num_nodes):\n",
    "        target_node_idx = random.randint(0, num_nodes - 1)\n",
    "        return target_node_idx\n",
    "    \n",
    "    def generate_root(self, num_nodes, target_node_idx):\n",
    "        node_indices = list(range(num_nodes))\n",
    "        node_indices.remove(target_node_idx)\n",
    "        root_idx = random.choice(node_indices)\n",
    "        return root_idx\n",
    "    \n",
    "    def process(self):\n",
    "        if not os.path.isfile(self.processed_paths[0]):\n",
    "            self.data = []\n",
    "            for _ in range(self.number_of_graphs):\n",
    "                family_tree = create_family_tree(self.generations)\n",
    "                graph_data = create_data_object(family_tree)\n",
    "\n",
    "                # Generate random labels for each node\n",
    "                target_node_idx = self.generate_labels(graph_data.num_nodes)\n",
    "\n",
    "                # Store the labels as an attribute of the graph_data\n",
    "                graph_data.target_node_idx = target_node_idx\n",
    "\n",
    "                root_idx = self.generate_root(graph_data.num_nodes, target_node_idx)\n",
    "\n",
    "                graph_data.root_idx = root_idx\n",
    "\n",
    "                self.data.append(graph_data)\n",
    "\n",
    "            torch.save(self.data, self.processed_paths[0])\n",
    "        else:\n",
    "            self.data = torch.load(self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3, 4], edge_index=[2, 6], edge_attr=[6], target_node_idx=0, root_idx=1)\n",
      "Data(x=[4, 4], edge_index=[2, 8], edge_attr=[8], target_node_idx=2, root_idx=0)\n"
     ]
    }
   ],
   "source": [
    "dataset = FamilyGraphDataset(root='/Users/meeslindeman/Library/Mobile Documents/com~apple~CloudDocs/Thesis/Code/data', number_of_graphs=100, generations=3)\n",
    "print(dataset[0])\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of nodes: 4.52\n"
     ]
    }
   ],
   "source": [
    "total_nodes = 0\n",
    "for i in range(0, len(dataset)):\n",
    "    total_nodes += dataset[i].x.shape[0]  # shape[0] gives the number of nodes in each graph\n",
    "\n",
    "average_nodes = total_nodes / len(dataset)\n",
    "print(\"Average number of nodes:\", average_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', tensor([[  0.,  84.,   2., 168.],\n",
      "        [  1.,  82.,   0., 177.],\n",
      "        [  0.,  59.,   3., 163.]]))\n",
      "('edge_index', tensor([[0, 1, 2, 1, 0, 2],\n",
      "        [1, 0, 0, 0, 1, 1]]))\n",
      "('edge_attr', tensor([0., 0., 1., 0., 0., 1.]))\n",
      "('target_node_idx', 0)\n",
      "('root_idx', 1)\n"
     ]
    }
   ],
   "source": [
    "graph = dataset[0]\n",
    "print(*graph, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, embedding_size, heads):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(num_node_features, embedding_size, edge_dim=1, heads=heads, concat=True)\n",
    "        self.conv2 = GATv2Conv(-1, embedding_size, edge_dim=1, heads=heads, concat=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        h = self.conv1(x=x, edge_index=edge_index, edge_attr=edge_attr)     \n",
    "        h = F.relu(h)\n",
    "\n",
    "        h = self.conv2(x=h, edge_index=edge_index, edge_attr=edge_attr)     \n",
    "        h = F.relu(h)\n",
    "\n",
    "        return h\n",
    "    \n",
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, embedding_size, heads):\n",
    "        super().__init__()\n",
    "        self.conv1 = TransformerConv(num_node_features, embedding_size, edge_dim=0, heads=heads, concat=True)\n",
    "        self.conv2 = TransformerConv(-1, embedding_size, edge_dim=0, heads=heads, concat=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        h = self.conv1(x=x, edge_index=edge_index, edge_attr=edge_attr)     \n",
    "        h = F.relu(h)\n",
    "\n",
    "        h = self.conv2(x=h, edge_index=edge_index, edge_attr=edge_attr)     \n",
    "        h = F.relu(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SenderDual(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size, temperature):\n",
    "        super(SenderDual, self).__init__()\n",
    "        self.num_node_features = dataset.num_node_features\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.temp = temperature\n",
    "\n",
    "        self.transform = Transform(self.num_node_features, embedding_size, heads)\n",
    "        self.gat = GAT(self.num_node_features, embedding_size, heads) \n",
    "        self.fc = nn.Linear((embedding_size * heads), hidden_size) \n",
    "\n",
    "    def forward(self, x, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        target_node_idx = data.target_node_idx\n",
    "\n",
    "        h_t = self.transform(data)\n",
    "\n",
    "        h_g = self.gat(data)\n",
    "\n",
    "        h = h_t + h_g\n",
    "\n",
    "        target_embedding = h[target_node_idx]           \n",
    "\n",
    "        output = self.fc(target_embedding)                           \n",
    "\n",
    "        return output.view(-1, self.hidden_size)\n",
    "\n",
    "class ReceiverDual(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size):\n",
    "        super(ReceiverDual, self).__init__()\n",
    "        self.num_node_features = dataset.num_node_features\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.transform = Transform(self.num_node_features, embedding_size, heads)\n",
    "        self.gat = GAT(self.num_node_features, embedding_size, heads) \n",
    "        self.fc = nn.Linear(hidden_size, (embedding_size * heads))\n",
    "\n",
    "    def forward(self, message, _input, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        h_t = self.transform(data)\n",
    "\n",
    "        h_g = self.gat(data)\n",
    "\n",
    "        h = h_t + h_g   \n",
    "\n",
    "        message_embedding = self.fc(message)        \n",
    "\n",
    "        dot_products = torch.matmul(h, message_embedding.t()).t()   \n",
    "\n",
    "        probabilities = F.log_softmax(dot_products, dim=1)                      \n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SenderGAT(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size, temperature):\n",
    "        super(SenderGAT, self).__init__()\n",
    "        self.num_node_features = dataset.num_node_features\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.temp = temperature\n",
    "\n",
    "        self.gat = GAT(self.num_node_features, embedding_size, heads) \n",
    "        self.fc = nn.Linear((embedding_size * heads), hidden_size) \n",
    "\n",
    "    def forward(self, x, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        target_node_idx = data.target_node_idx\n",
    "\n",
    "        h = self.gat(data)\n",
    "\n",
    "        target_embedding = h[target_node_idx]           \n",
    "\n",
    "        output = self.fc(target_embedding)                           \n",
    "\n",
    "        return output.view(-1, self.hidden_size)\n",
    "\n",
    "class ReceiverGAT(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size):\n",
    "        super(ReceiverGAT, self).__init__()\n",
    "        self.num_node_features = dataset.num_node_features\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gat = GAT(self.num_node_features, embedding_size, heads)\n",
    "        self.fc = nn.Linear(hidden_size, (embedding_size * heads))\n",
    "\n",
    "    def forward(self, message, _input, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        h = self.gat(data)   \n",
    "\n",
    "        message_embedding = self.fc(message)                 \n",
    "\n",
    "        dot_products = torch.matmul(h, message_embedding.t()).t()   \n",
    "\n",
    "        probabilities = F.log_softmax(dot_products, dim=1)                      \n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SenderTransform(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size, temperature):\n",
    "        super(SenderTransform, self).__init__()\n",
    "        self.num_node_features = dataset.num_node_features\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.temp = temperature\n",
    "          \n",
    "        self.transform = Transform(self.num_node_features, embedding_size, heads) \n",
    "        self.fc = nn.Linear((embedding_size * heads), hidden_size) \n",
    "\n",
    "    def forward(self, x, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        target_node_idx = data.target_node_idx\n",
    "\n",
    "        h = self.transform(data)\n",
    "\n",
    "        target_embedding = h[target_node_idx]           \n",
    "\n",
    "        output = self.fc(target_embedding)                           \n",
    "\n",
    "        return output.view(-1, self.hidden_size)\n",
    "\n",
    "class ReceiverTransform(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size):\n",
    "        super(ReceiverTransform, self).__init__()\n",
    "        self.num_node_features = dataset.num_node_features\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.transform = Transform(self.num_node_features, embedding_size, heads)\n",
    "        self.fc = nn.Linear(hidden_size, (embedding_size * heads))\n",
    "\n",
    "    def forward(self, message, _input, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        h = self.transform(data)   \n",
    "\n",
    "        message_embedding = self.fc(message)                 \n",
    "\n",
    "        dot_products = torch.matmul(h, message_embedding.t()).t()   \n",
    "\n",
    "        probabilities = F.log_softmax(dot_products, dim=1)                      \n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouput for a single graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = \"transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if agents == \"dual\":\n",
    "    sender = SenderDual(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size, temperature=opts.gs_tau) \n",
    "    receiver = ReceiverDual(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size)\n",
    "elif agents == \"transform\":\n",
    "    sender = SenderTransform(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size, temperature=opts.gs_tau) \n",
    "    receiver = ReceiverTransform(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size) \n",
    "elif agents == \"gat\":\n",
    "    sender = SenderGAT(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size, temperature=opts.gs_tau) \n",
    "    receiver = ReceiverGAT(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size) \n",
    "else:\n",
    "    print(\"Invalid agent type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3, 4], edge_index=[2, 6], edge_attr=[6], target_node_idx=0, root_idx=1)\n",
      "Sender's message: tensor([[ 20.4228,  27.5035, -29.3112,  27.5490, -48.8582,   6.6548,   8.7320,\n",
      "         -56.9502,  -4.3411, -16.9194,  -4.7595,   1.9552,  65.5158, -13.4471,\n",
      "          24.2548,  10.5091,  -9.9861, -11.5677,   5.6719, -18.4157]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Sender's shape:  torch.Size([1, 20])\n",
      "Receiver's output: tensor([[   0.0000,  -37.8854, -974.5131]], grad_fn=<LogSoftmaxBackward0>)\n",
      "Receiver's shape:  torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])\n",
    "\n",
    "# Sender produces a message\n",
    "sender_output = sender(None, dataset[0])\n",
    "print(\"Sender's message:\", sender_output)\n",
    "print(\"Sender's shape: \", sender_output.shape)\n",
    "\n",
    "# Receiver tries to identify the target node\n",
    "receiver_output = receiver(sender_output, None, dataset[0])\n",
    "print(\"Receiver's output:\", receiver_output)\n",
    "print(\"Receiver's shape: \", receiver_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[4, 4], edge_index=[2, 8], edge_attr=[8], target_node_idx=2, root_idx=0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x8 and 6x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Sender produces a message\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m sender_output \u001b[38;5;241m=\u001b[39m \u001b[43msender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sender_output)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, sender_output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m, in \u001b[0;36mSenderTransform.forward\u001b[0;34m(self, x, _aux_input)\u001b[0m\n\u001b[1;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m _aux_input\n\u001b[1;32m     15\u001b[0m target_node_idx \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtarget_node_idx\n\u001b[0;32m---> 17\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m target_embedding \u001b[38;5;241m=\u001b[39m h[target_node_idx]           \n\u001b[1;32m     21\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(target_embedding)                           \n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m     29\u001b[0m     x, edge_index, edge_attr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39medge_attr\n\u001b[0;32m---> 31\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[1;32m     32\u001b[0m     h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(h)\n\u001b[1;32m     34\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x\u001b[38;5;241m=\u001b[39mh, edge_index\u001b[38;5;241m=\u001b[39medge_index, edge_attr\u001b[38;5;241m=\u001b[39medge_attr)     \n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch_geometric/nn/conv/transformer_conv.py:177\u001b[0m, in \u001b[0;36mTransformerConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    174\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_value(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, H, C)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# propagate_type: (query: Tensor, key:Tensor, value: Tensor, edge_attr: OptTensor) # noqa\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                     \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:463\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m         msg_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 463\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmsg_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    465\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (msg_kwargs, ), out)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch_geometric/nn/conv/transformer_conv.py:212\u001b[0m, in \u001b[0;36mTransformerConv.message\u001b[0;34m(self, query_i, key_j, value_j, edge_attr, index, ptr, size_i)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_edge \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads,\n\u001b[1;32m    213\u001b[0m                                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n\u001b[1;32m    214\u001b[0m     key_j \u001b[38;5;241m=\u001b[39m key_j \u001b[38;5;241m+\u001b[39m edge_attr\n\u001b[1;32m    216\u001b[0m alpha \u001b[38;5;241m=\u001b[39m (query_i \u001b[38;5;241m*\u001b[39m key_j)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch_geometric/nn/dense/linear.py:130\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x8 and 6x20)"
     ]
    }
   ],
   "source": [
    "print(dataset[1])\n",
    "\n",
    "# Sender produces a message\n",
    "sender_output = sender(None, dataset[1])\n",
    "print(\"Sender's message:\", sender_output)\n",
    "print(\"Sender's shape: \", sender_output.shape)\n",
    "\n",
    "# Receiver tries to identify the target node\n",
    "receiver_output = receiver(sender_output, None, dataset[1])\n",
    "print(\"Receiver's output:\", receiver_output)\n",
    "print(\"Receiver's shape: \", receiver_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can already see that the class has updated `self.weight` to the first graph's edge size ([6] in my case, for 3 nodes). You have to compare `dataset[0]` and `dataset[1]`. If they are different size graphs the error will be something like `RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x8 and 6x20)` where (6x20) is (edge_attributs x embedding_size) from `dataset[0]` and (1x8) is from `dataset[1]`, which in my case had 8 attributes. Notice that embedding size is set in `Options()` but gets multiplied by the number of heads. \n",
    "\n",
    "So the first graph is passed correctly but the second is not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sender's shape: torch.Size([1, 5, 100])\n",
      "Receiver's shape: torch.Size([1, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "sender_gs = core.RnnSenderGS(sender, opts.vocab_size, opts.embedding_size, opts.hidden_size, max_len=opts.max_len, temperature=opts.gs_tau, cell=opts.sender_cell)\n",
    "receiver_gs = core.RnnReceiverGS(receiver, opts.vocab_size, opts.embedding_size, opts.hidden_size, cell=opts.sender_cell)\n",
    "\n",
    "# Sender produces a message\n",
    "sender_output = sender_gs(None, dataset[0])\n",
    "#print(\"Sender's message:\", sender_output)\n",
    "print(\"Sender's shape:\", sender_output.shape) # batch size x max_len+1 x vocab size\n",
    "\n",
    "# Receiver tries to identify the target node\n",
    "receiver_output = receiver_gs(sender_output, None, dataset[0])\n",
    "#print(\"Receiver's output:\", receiver_output)\n",
    "print(\"Receiver's shape:\", receiver_output.shape) # nodes x max_len+1 x num_classes (nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6853, grad_fn=<MeanBackward0>)\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "def loss_nll(\n",
    "    _sender_input, _message, _receiver_input, receiver_output, labels, _aux_input):\n",
    "    \"\"\"\n",
    "    NLL loss - differentiable and can be used with both GS and Reinforce\n",
    "    \"\"\"\n",
    "    labels = torch.tensor([labels]).long()\n",
    "    nll = F.nll_loss(receiver_output, labels, reduction=\"none\")\n",
    "    acc = (labels == receiver_output.argmax(dim=1)).float().mean()\n",
    "    return nll, {\"acc\": acc}\n",
    "\n",
    "game = core.SenderReceiverRnnGS(sender_gs, receiver_gs, loss_nll)\n",
    "\n",
    "loss, interaction = game(sender_input=None, labels=graph.target_node_idx, receiver_input=None, aux_input=graph)\n",
    "print(loss)\n",
    "print(\"====================================\")\n",
    "#print(interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collater:\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_size: int,  # the number of graphs for a game\n",
    "        dataset: Union[Dataset, Sequence[BaseData], DatasetAdapter],\n",
    "        follow_batch: Optional[List[str]] = None,\n",
    "        exclude_keys: Optional[List[str]] = None,\n",
    "    ):\n",
    "        self.game_size = game_size\n",
    "        self.dataset = dataset\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "    def __call__(self, batch: List[Any]) -> Any:\n",
    "        elem = batch[0]\n",
    "        if isinstance(elem, BaseData):\n",
    "            batch = batch[:((len(batch) // self.game_size) * self.game_size)]  # we throw away the last batch_size % game_size\n",
    "            batch = Batch.from_data_list(\n",
    "                batch,\n",
    "                follow_batch=self.follow_batch,\n",
    "                exclude_keys=self.exclude_keys,\n",
    "            )\n",
    "            # we return a tuple (sender_input, labels, receiver_input, aux_input)\n",
    "            # we use aux_input to store minibatch of graphs\n",
    "            return (\n",
    "                torch.zeros(len(batch) // self.game_size, 1),  # we don't need sender_input --> create a fake one\n",
    "                batch.target_node_idx,  # the target is aways the first graph among game_size graphs\n",
    "                None,  # we don't care about receiver_input\n",
    "                batch  # this is a compact data for batch_size graphs \n",
    "            )\n",
    "\n",
    "        raise TypeError(f\"DataLoader found invalid type: '{type(elem)}'\")\n",
    "\n",
    "    def collate_fn(self, batch: List[Any]) -> Any:\n",
    "        if isinstance(self.dataset, OnDiskDataset):\n",
    "            return self(self.dataset.multi_get(batch))\n",
    "        return self(batch)\n",
    "\n",
    "\n",
    "class DataLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_size: int,  # the number of graphs for a game\n",
    "        dataset: Union[Dataset, Sequence[BaseData], DatasetAdapter],\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        follow_batch: Optional[List[str]] = None,\n",
    "        exclude_keys: Optional[List[str]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.game_size = game_size\n",
    "        # Remove for PyTorch Lightning:\n",
    "        kwargs.pop('collate_fn', None)\n",
    "\n",
    "        # Save for PyTorch Lightning < 1.6:\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "        self.collator = Collater(game_size, dataset, follow_batch, exclude_keys)\n",
    "\n",
    "        if isinstance(dataset, OnDiskDataset):\n",
    "            dataset = range(len(dataset))\n",
    "\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            shuffle,\n",
    "            collate_fn=self.collator.collate_fn,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 80\n",
      "Validation set length: 20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the lengths of the training and validation sets\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(game_size=1, dataset=train_data, batch_size=opts.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(game_size=1, dataset=val_data, batch_size=opts.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs for batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[3, 4], edge_index=[2, 6], edge_attr=[6], target_node_idx=[1], root_idx=[1], batch=[3], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[3, 4], edge_index=[2, 6], edge_attr=[6], target_node_idx=[1], root_idx=[1], batch=[3], ptr=[2])\n",
      "Sender's shape:  torch.Size([1, 20])\n",
      "Receiver's shape:  torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "print(batch[3])\n",
    "\n",
    "# Sender produces a message\n",
    "sender_output = sender(None, batch[3])\n",
    "#print(\"Sender's message:\", sender_output)\n",
    "print(\"Sender's shape: \", sender_output.shape)\n",
    "\n",
    "# Receiver tries to identify the target node\n",
    "receiver_output = receiver(sender_output, None, batch[3])\n",
    "#print(\"Receiver's output:\", receiver_output)\n",
    "print(\"Receiver's shape: \", receiver_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sender's shape: torch.Size([1, 5, 100])\n",
      "Receiver's output shape: torch.Size([1, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# Sender produces a message\n",
    "sender_output = sender_gs(None, batch[3])\n",
    "#print(\"Sender's message:\", sender_output)\n",
    "print(\"Sender's shape:\", sender_output.shape) # batch size x max_len+1 x vocab size\n",
    "\n",
    "# Receiver tries to identify the target node\n",
    "receiver_output = receiver_gs(sender_output, None, batch[3])\n",
    "#print(\"Receiver's output:\", receiver_output)\n",
    "print(\"Receiver's output shape:\", receiver_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6589, grad_fn=<MeanBackward0>)\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "def loss_nll(\n",
    "    _sender_input, _message, _receiver_input, receiver_output, labels, _aux_input):\n",
    "    \"\"\"\n",
    "    NLL loss - differentiable and can be used with both GS and Reinforce\n",
    "    \"\"\"\n",
    "    nll = F.nll_loss(receiver_output, labels, reduction=\"none\")\n",
    "    acc = (labels == receiver_output.argmax(dim=1)).float().mean()\n",
    "    return nll, {\"acc\": acc}\n",
    "\n",
    "game = core.SenderReceiverRnnGS(sender_gs, receiver_gs, loss_nll)\n",
    "\n",
    "loss, interaction = game(*batch)\n",
    "print(loss)\n",
    "print(\"====================================\")\n",
    "#print(interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x12 and 6x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 19\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(game\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     12\u001b[0m     game\u001b[38;5;241m=\u001b[39mgame, \n\u001b[1;32m     13\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[core\u001b[38;5;241m.\u001b[39mConsoleLogger(as_json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, print_train_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)]\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/trainers.py:273\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    271\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 273\u001b[0m train_loss, train_interaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    276\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_end(train_loss, train_interaction, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/trainers.py:216\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m context \u001b[38;5;241m=\u001b[39m autocast() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;28;01melse\u001b[39;00m nullcontext()\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m--> 216\u001b[0m     optimized_loss, interaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_freq \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;66;03m# throughout EGG, we minimize _mean_ loss, not sum\u001b[39;00m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;66;03m# hence, we need to account for that when aggregating grads\u001b[39;00m\n\u001b[1;32m    221\u001b[0m         optimized_loss \u001b[38;5;241m=\u001b[39m optimized_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_freq\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/gs_wrappers.py:493\u001b[0m, in \u001b[0;36mSenderReceiverRnnGS.forward\u001b[0;34m(self, sender_input, labels, receiver_input, aux_input)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sender_input, labels, receiver_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, aux_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 493\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m     receiver_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreceiver(message, receiver_input, aux_input)\n\u001b[1;32m    496\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/gs_wrappers.py:334\u001b[0m, in \u001b[0;36mRnnSenderGS.forward\u001b[0;34m(self, x, aux_input)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, aux_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 334\u001b[0m     prev_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     prev_c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(prev_hidden)  \u001b[38;5;66;03m# only for LSTM\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     e_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msos_embedding] \u001b[38;5;241m*\u001b[39m prev_hidden\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mSenderDual.forward\u001b[0;34m(self, x, _aux_input)\u001b[0m\n\u001b[1;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m _aux_input\n\u001b[1;32m     16\u001b[0m target_node_idx \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtarget_node_idx\n\u001b[0;32m---> 18\u001b[0m h_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m h_g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgat(data)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# h = h_t + h_g\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m     29\u001b[0m     x, edge_index, edge_attr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39medge_attr\n\u001b[0;32m---> 31\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[1;32m     32\u001b[0m     h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(h)\n\u001b[1;32m     34\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x\u001b[38;5;241m=\u001b[39mh, edge_index\u001b[38;5;241m=\u001b[39medge_index, edge_attr\u001b[38;5;241m=\u001b[39medge_attr)     \n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch_geometric/nn/conv/transformer_conv.py:177\u001b[0m, in \u001b[0;36mTransformerConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    174\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_value(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, H, C)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# propagate_type: (query: Tensor, key:Tensor, value: Tensor, edge_attr: OptTensor) # noqa\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                     \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:463\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m         msg_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 463\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmsg_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    465\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (msg_kwargs, ), out)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch_geometric/nn/conv/transformer_conv.py:212\u001b[0m, in \u001b[0;36mTransformerConv.message\u001b[0;34m(self, query_i, key_j, value_j, edge_attr, index, ptr, size_i)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_edge \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads,\n\u001b[1;32m    213\u001b[0m                                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n\u001b[1;32m    214\u001b[0m     key_j \u001b[38;5;241m=\u001b[39m key_j \u001b[38;5;241m+\u001b[39m edge_attr\n\u001b[1;32m    216\u001b[0m alpha \u001b[38;5;241m=\u001b[39m (query_i \u001b[38;5;241m*\u001b[39m key_j)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch_geometric/nn/dense/linear.py:130\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x12 and 6x20)"
     ]
    }
   ],
   "source": [
    "game = core.SenderReceiverRnnGS(sender_gs, receiver_gs, loss_nll)\n",
    "\n",
    "options = core.init(params=['--random_seed=7', \n",
    "                         '--lr=1e-2',   \n",
    "                         f'--batch_size={opts.batch_size}',\n",
    "                         '--optimizer=adam',\n",
    "                         '--update_freq=10'])\n",
    "\n",
    "optimizer = torch.optim.Adam(game.parameters())\n",
    "\n",
    "trainer = core.Trainer(\n",
    "    game=game, \n",
    "    optimizer=optimizer, \n",
    "    train_data=train_loader,\n",
    "    validation_data=val_loader, \n",
    "    callbacks=[core.ConsoleLogger(as_json=True, print_train_loss=True)]\n",
    ")\n",
    "\n",
    "trainer.train(n_epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
