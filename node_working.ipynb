{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import egg.core as core\n",
    "from options import Options\n",
    "opts = Options()\n",
    "\n",
    "from typing import Any, List, Optional, Sequence, Union\n",
    "\n",
    "import torch.utils.data\n",
    "from torch_geometric.data import Batch, Dataset, Data\n",
    "from torch_geometric.data.data import BaseData\n",
    "from torch_geometric.data.datapipes import DatasetAdapter\n",
    "from torch_geometric.data.on_disk_dataset import OnDiskDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from torch_geometric.data import Dataset\n",
    "from graph.build import create_family_tree, create_data_object\n",
    "\n",
    "class FamilyGraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for generating family graph data.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory path.\n",
    "        number_of_graphs (int): Number of graphs to generate.\n",
    "        generations (int): Number of generations in each family tree.\n",
    "\n",
    "    Returns:\n",
    "        Data(x=[8, 2], edge_index=[2, 20], edge_attr=[20], labels=[8])\n",
    "    \"\"\"\n",
    "    def __init__(self, root, number_of_graphs, generations, transform=None, pre_transform=None):\n",
    "        self.number_of_graphs = number_of_graphs\n",
    "        self.generations = generations\n",
    "        super(FamilyGraphDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data = None\n",
    "        self.process()\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['family_graphs.pt']\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def generate_labels(self, num_nodes):\n",
    "        target_node_idx = random.randint(0, num_nodes - 1)\n",
    "        return target_node_idx\n",
    "    \n",
    "    def generate_root(self, num_nodes, target_node_idx):\n",
    "        node_indices = list(range(num_nodes))\n",
    "        node_indices.remove(target_node_idx)\n",
    "        root_idx = random.choice(node_indices)\n",
    "        return root_idx\n",
    "    \n",
    "    def process(self):\n",
    "        if not os.path.isfile(self.processed_paths[0]):\n",
    "            self.data = []\n",
    "            for _ in range(self.number_of_graphs):\n",
    "                family_tree = create_family_tree(self.generations)\n",
    "                graph_data = create_data_object(family_tree)\n",
    "\n",
    "                # Generate random labels for each node\n",
    "                target_node_idx = self.generate_labels(graph_data.num_nodes)\n",
    "\n",
    "                # Store the labels as an attribute of the graph_data\n",
    "                graph_data.target_node_idx = target_node_idx\n",
    "\n",
    "                root_idx = self.generate_root(graph_data.num_nodes, target_node_idx)\n",
    "\n",
    "                graph_data.root_idx = root_idx\n",
    "\n",
    "                self.data.append(graph_data)\n",
    "\n",
    "            torch.save(self.data, self.processed_paths[0])\n",
    "        else:\n",
    "            self.data = torch.load(self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[4, 2], edge_index=[2, 10], edge_attr=[10, 3], target_node_idx=3, root_idx=1)\n",
      "Data(x=[4, 2], edge_index=[2, 10], edge_attr=[10, 3], target_node_idx=2, root_idx=3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = FamilyGraphDataset(root='/Users/meeslindeman/Library/Mobile Documents/com~apple~CloudDocs/Thesis/Code/data', number_of_graphs=5000, generations=2)\n",
    "print(dataset[0])\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph.dataset import FamilyGraphDataset\n",
    "dataset = FamilyGraphDataset(root=f'data/gens={opts.generations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(dataset[0].num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph.draw import draw_graph\n",
    "\n",
    "# plot = draw_graph(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of nodes: 4.0\n"
     ]
    }
   ],
   "source": [
    "total_nodes = 0\n",
    "for i in range(0, len(dataset)):\n",
    "    total_nodes += dataset[i].x.shape[0]  # shape[0] gives the number of nodes in each graph\n",
    "\n",
    "average_nodes = total_nodes / len(dataset)\n",
    "print(\"Average number of nodes:\", average_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].edge_attr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', tensor([[ 0., 92.],\n",
      "        [ 1., 96.],\n",
      "        [ 0., 69.],\n",
      "        [ 0., 64.]]))\n",
      "('edge_index', tensor([[0, 2, 0, 3, 0, 1, 2, 1, 3, 1],\n",
      "        [1, 0, 2, 0, 3, 0, 1, 2, 1, 3]]))\n",
      "('edge_attr', tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]]))\n",
      "('target_node_idx', 1)\n",
      "('root_idx', 2)\n"
     ]
    }
   ],
   "source": [
    "graph = dataset[0]\n",
    "print(*graph, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, embedding_size, heads):\n",
    "        super().__init__()\n",
    "        self.out_heads = 1\n",
    "\n",
    "        self.conv1 = GATv2Conv(num_node_features, embedding_size, edge_dim=3, heads=heads, concat=True)\n",
    "        self.conv2 = GATv2Conv(-1, embedding_size, edge_dim=3, heads=self.out_heads, concat=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        h = self.conv1(x=x, edge_index=edge_index, edge_attr=edge_attr)     \n",
    "        h = F.leaky_relu(h)\n",
    "\n",
    "        h = self.conv2(x=h, edge_index=edge_index, edge_attr=edge_attr)     \n",
    "\n",
    "        return h\n",
    "    \n",
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, embedding_size, heads):\n",
    "        super().__init__()\n",
    "        self.out_heads = 1\n",
    "\n",
    "        self.conv1 = TransformerConv(num_node_features, embedding_size, edge_dim=3, heads=heads, concat=True)\n",
    "        self.conv2 = TransformerConv(-1, embedding_size, edge_dim=3, heads=self.out_heads, concat=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        h = self.conv1(x=x, edge_index=edge_index, edge_attr=edge_attr)     \n",
    "        h = F.leaky_relu(h)\n",
    "\n",
    "        h = self.conv2(x=h, edge_index=edge_index, edge_attr=edge_attr)     \n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SenderDual(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size, temperature):\n",
    "        super(SenderDual, self).__init__()\n",
    "        self.num_node_features = dataset.num_node_features\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.temp = temperature\n",
    "\n",
    "        self.transform = Transform(self.num_node_features, embedding_size, heads)\n",
    "        self.gat = GAT(self.num_node_features, embedding_size, heads) \n",
    "        self.fc = nn.Linear(embedding_size, hidden_size) \n",
    "\n",
    "    def forward(self, x, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        batch_ptr = data.ptr\n",
    "        target_node_idx = data.target_node_idx\n",
    "\n",
    "        h_t = self.transform(data)\n",
    "        h_g = self.gat(data)\n",
    "        h = h_t + h_g\n",
    "\n",
    "        adjusted_target_node_idx = target_node_idx + batch_ptr[:-1]\n",
    "\n",
    "        target_embedding = h[adjusted_target_node_idx]       \n",
    "\n",
    "        output = self.fc(target_embedding)                           \n",
    "\n",
    "        return output.view(-1, self.hidden_size)\n",
    "\n",
    "class ReceiverDual(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size):\n",
    "        super(ReceiverDual, self).__init__()\n",
    "        self.num_node_features = dataset.num_node_features\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.transform = Transform(self.num_node_features, embedding_size, heads)\n",
    "        self.gat = GAT(self.num_node_features, embedding_size, heads) \n",
    "        self.fc = nn.Linear(hidden_size, embedding_size)\n",
    "\n",
    "    def forward(self, message, _input, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        h_t = self.transform(data)\n",
    "        h_g = self.gat(data)\n",
    "        h = h_t + h_g   \n",
    "\n",
    "        # Reshape h for batched operation\n",
    "        batch_size = data.num_graphs  # Assuming this attribute is available\n",
    "        num_nodes_per_graph = data.num_nodes // batch_size  # Assuming equal number of nodes in each graph\n",
    "        h = h.view(batch_size, num_nodes_per_graph, -1)\n",
    "\n",
    "        message_embedding = self.fc(message)  \n",
    "        message_embedding = message_embedding.view(batch_size, -1, 1)\n",
    "\n",
    "        dot_products = torch.bmm(h, message_embedding).squeeze(-1)   \n",
    "\n",
    "        probabilities = F.log_softmax(dot_products, dim=1)                      \n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SenderGAT(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size, temperature):\n",
    "        super(SenderGAT, self).__init__()\n",
    "        self.num_node_features = dataset.num_node_features\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.temp = temperature\n",
    "\n",
    "        self.gat = GAT(self.num_node_features, embedding_size, heads) \n",
    "        self.fc = nn.Linear(embedding_size, hidden_size) \n",
    "\n",
    "    def forward(self, x, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        target_node_idx = data.target_node_idx\n",
    "\n",
    "        h = self.gat(data)\n",
    "\n",
    "        target_embedding = h[target_node_idx]           \n",
    "\n",
    "        output = self.fc(target_embedding)                           \n",
    "\n",
    "        return output.view(-1, self.hidden_size)\n",
    "\n",
    "class ReceiverGAT(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size):\n",
    "        super(ReceiverGAT, self).__init__()\n",
    "        self.num_node_features = dataset.num_node_features\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gat = GAT(self.num_node_features, embedding_size, heads)\n",
    "        self.fc = nn.Linear(hidden_size, embedding_size)\n",
    "\n",
    "    def forward(self, message, _input, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        h = self.gat(data)   \n",
    "\n",
    "        message_embedding = self.fc(message)                 \n",
    "\n",
    "        dot_products = torch.matmul(h, message_embedding.t()).t()   \n",
    "\n",
    "        probabilities = F.log_softmax(dot_products, dim=1)                      \n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SenderTransform(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size, temperature):\n",
    "        super(SenderTransform, self).__init__()\n",
    "        self.num_node_features = dataset.num_node_features\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.temp = temperature\n",
    "          \n",
    "        self.transform = Transform(self.num_node_features, embedding_size, heads) \n",
    "        self.fc = nn.Linear(embedding_size, hidden_size) \n",
    "\n",
    "    def forward(self, x, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        target_node_idx = data.target_node_idx\n",
    "\n",
    "        h = self.transform(data)\n",
    "\n",
    "        target_embedding = h[target_node_idx]           \n",
    "\n",
    "        output = self.fc(target_embedding)                           \n",
    "\n",
    "        return output.view(-1, self.hidden_size)\n",
    "\n",
    "class ReceiverTransform(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size):\n",
    "        super(ReceiverTransform, self).__init__()\n",
    "        self.num_node_features = dataset.num_node_features\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.transform = Transform(self.num_node_features, embedding_size, heads)\n",
    "        self.fc = nn.Linear(hidden_size, embedding_size)\n",
    "\n",
    "    def forward(self, message, _input, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        h = self.transform(data)   \n",
    "\n",
    "        message_embedding = self.fc(message)                 \n",
    "\n",
    "        dot_products = torch.matmul(h, message_embedding.t()).t()   \n",
    "\n",
    "        probabilities = F.log_softmax(dot_products, dim=1)                      \n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SenderRel(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size, temperature):\n",
    "        super(SenderRel, self).__init__()\n",
    "        self.num_node_features = 4\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.temp = temperature\n",
    "\n",
    "        self.transform = Transform(self.num_node_features, embedding_size, heads)\n",
    "        self.gat = GAT(self.num_node_features, embedding_size, heads) \n",
    "        self.fc = nn.Linear(embedding_size * 2, hidden_size) \n",
    "\n",
    "    def forward(self, x, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        target_node_idx, root_idx = data.target_node_idx, data.root_idx\n",
    "\n",
    "        h_t = self.transform(data)\n",
    "\n",
    "        h_g = self.gat(data)\n",
    "\n",
    "        h = h_t + h_g                       \n",
    "\n",
    "        target = h[target_node_idx].squeeze()\n",
    "        root = h[root_idx].squeeze()\n",
    "\n",
    "        target_embedding = torch.cat((target, root))\n",
    "\n",
    "        output = self.fc(target_embedding)                           \n",
    "\n",
    "        return output.view(-1, self.hidden_size)\n",
    "\n",
    "class ReceiverRel(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_size):\n",
    "        super(ReceiverRel, self).__init__()\n",
    "        self.num_node_features = 4\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.transform = Transform(self.num_node_features, embedding_size, heads)\n",
    "        self.gat = GAT(self.num_node_features, embedding_size, heads) \n",
    "        self.fc = nn.Linear(hidden_size, embedding_size)\n",
    "\n",
    "    def forward(self, message, _input, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        h_t = self.transform(data)\n",
    "\n",
    "        h_g = self.gat(data)\n",
    "\n",
    "        h = h_t + h_g   \n",
    "\n",
    "        message_embedding = self.fc(message)        \n",
    "\n",
    "        dot_products = torch.matmul(h, message_embedding.t()).t()   \n",
    "\n",
    "        probabilities = F.log_softmax(dot_products, dim=1)                      \n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouput for a single graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = \"dual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if agents == \"dual\":\n",
    "    sender = SenderDual(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size, temperature=opts.gs_tau) \n",
    "    receiver = ReceiverDual(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size)\n",
    "elif agents == \"transform\":\n",
    "    sender = SenderTransform(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size, temperature=opts.gs_tau) \n",
    "    receiver = ReceiverTransform(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size) \n",
    "elif agents == \"gat\":\n",
    "    sender = SenderGAT(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size, temperature=opts.gs_tau) \n",
    "    receiver = ReceiverGAT(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size) \n",
    "elif agents == \"rel\":\n",
    "    sender = SenderRel(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size, temperature=opts.gs_tau) \n",
    "    receiver = ReceiverRel(embedding_size=opts.embedding_size, heads=opts.heads, hidden_size=opts.hidden_size) \n",
    "else:\n",
    "    print(\"Invalid agent type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4., 2., 6., 2., 6., 2.]])\n"
     ]
    }
   ],
   "source": [
    "def process_graph_to_sequence(graph):\n",
    "    # Vocabulary\n",
    "    vocab = {'(': 0, ')': 1, 'male': 2, 'female': 3, 'married-to': 4, 'child-of': 5, 'gave-birth-to': 6}\n",
    "\n",
    "    # Function to get node information\n",
    "    def get_node_info(graph):\n",
    "        node_info = {}\n",
    "        relationship_types = {tuple([1., 0., 0.]): 'married-to', tuple([0., 1., 0.]): 'child-of', tuple([0., 0., 1.]): 'gave-birth-to'}\n",
    "\n",
    "        for index, features_tensor in enumerate(graph.x):\n",
    "            features = features_tensor.tolist()\n",
    "            gender = 'male' if features[0] == 0 else 'female'\n",
    "            features_dict = {'gender': gender, 'features': features}\n",
    "\n",
    "            relationships = []\n",
    "            for i in range(graph.edge_index.shape[1]):\n",
    "                if graph.edge_index[0, i] == index:\n",
    "                    target_node = graph.edge_index[1, i]\n",
    "                    rel_type = relationship_types[tuple(graph.edge_attr[i].tolist())]\n",
    "                    relationships.append({'node': target_node.item(), 'relationship': rel_type})\n",
    "\n",
    "            features_dict['relationships'] = relationships\n",
    "            node_info[index] = features_dict\n",
    "\n",
    "        return node_info\n",
    "\n",
    "    # Function to sort tree\n",
    "    def sort_tree(node_data, start_node):\n",
    "        visited = {start_node}\n",
    "\n",
    "        def get_sorted_children(node_index):\n",
    "            relationships = [r for r in node_data[node_index]['relationships'] if r['node'] not in visited]\n",
    "            married_child = [child['node'] for child in relationships if child['relationship'] == 'married-to']\n",
    "            other_children = sorted([child['node'] for child in relationships if child['node'] not in married_child], \n",
    "                                    key=lambda x: node_data[x]['gender'] == 'male')\n",
    "            visited.update(married_child + other_children)\n",
    "            return married_child + other_children\n",
    "\n",
    "        def build_tree(node_index):\n",
    "            children_indices = get_sorted_children(node_index)\n",
    "            children = [build_tree(child_index) for child_index in children_indices]\n",
    "            return {\"index\": node_index, \"children\": children}\n",
    "\n",
    "        return build_tree(start_node)\n",
    "\n",
    "    # Function to build sequence\n",
    "    def build_sequence(tree, node_data, vocab):\n",
    "        def node_sequence(node):\n",
    "            sequence = [vocab[node_data[node['index']]['gender']]]\n",
    "            for child in node['children']:\n",
    "                relationship = next(r['relationship'] for r in node_data[node['index']]['relationships'] if r['node'] == child['index'])\n",
    "                sequence.append(vocab[relationship])\n",
    "\n",
    "                if child['children']:\n",
    "                    sequence.append(vocab['('])  # Opening parenthesis\n",
    "                    sequence.extend(node_sequence(child))\n",
    "                    sequence.append(vocab[')'])  # Closing parenthesis\n",
    "                else:\n",
    "                    sequence.extend(node_sequence(child))\n",
    "\n",
    "            return sequence\n",
    "\n",
    "        sequence = node_sequence(tree)\n",
    "        return torch.tensor([sequence], dtype=torch.float32)\n",
    "\n",
    "    # Process graph to sequence\n",
    "    node_info = get_node_info(graph)\n",
    "    tree = sort_tree(node_info, graph.target_node_idx)\n",
    "    sequence_tensor = build_sequence(tree, node_info, vocab)\n",
    "\n",
    "    return sequence_tensor\n",
    "\n",
    "# Example usage (assuming graph and graph.target_node_idx are defined)\n",
    "sequence_tensor = process_graph_to_sequence(graph)\n",
    "print(sequence_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collater:\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_size: int,  # the number of graphs for a game\n",
    "        dataset: Union[Dataset, Sequence[BaseData], DatasetAdapter],\n",
    "        follow_batch: Optional[List[str]] = None,\n",
    "        exclude_keys: Optional[List[str]] = None,\n",
    "    ):\n",
    "        self.game_size = game_size\n",
    "        self.dataset = dataset\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "    def __call__(self, batch: List[Any]) -> Any:\n",
    "        elem = batch[0]\n",
    "        if isinstance(elem, BaseData):\n",
    "            batch = batch[:((len(batch) // self.game_size) * self.game_size)]  # we throw away the last batch_size % game_size\n",
    "            batch = Batch.from_data_list(\n",
    "                batch,\n",
    "                follow_batch=self.follow_batch,\n",
    "                exclude_keys=self.exclude_keys,\n",
    "            )\n",
    "            # we return a tuple (sender_input, labels, receiver_input, aux_input)\n",
    "            # we use aux_input to store minibatch of graphs\n",
    "            return (\n",
    "                batch.x.view(1, -1),  # we don't need sender_input --> create a fake one\n",
    "                batch.target_node_idx,  # the target is aways the first graph among game_size graphs\n",
    "                None,  # we don't care about receiver_input\n",
    "                batch  # this is a compact data for batch_size graphs \n",
    "            )\n",
    "\n",
    "        raise TypeError(f\"DataLoader found invalid type: '{type(elem)}'\")\n",
    "\n",
    "    def collate_fn(self, batch: List[Any]) -> Any:\n",
    "        if isinstance(self.dataset, OnDiskDataset):\n",
    "            return self(self.dataset.multi_get(batch))\n",
    "        return self(batch)\n",
    "\n",
    "\n",
    "class DataLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_size: int,  # the number of graphs for a game\n",
    "        dataset: Union[Dataset, Sequence[BaseData], DatasetAdapter],\n",
    "        batch_size: int = 1,\n",
    "        shuffle: bool = False,\n",
    "        follow_batch: Optional[List[str]] = None,\n",
    "        exclude_keys: Optional[List[str]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.game_size = game_size\n",
    "        # Remove for PyTorch Lightning:\n",
    "        kwargs.pop('collate_fn', None)\n",
    "\n",
    "        # Save for PyTorch Lightning < 1.6:\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "\n",
    "        self.collator = Collater(game_size, dataset, follow_batch, exclude_keys)\n",
    "\n",
    "        if isinstance(dataset, OnDiskDataset):\n",
    "            dataset = range(len(dataset))\n",
    "\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            shuffle,\n",
    "            collate_fn=self.collator.collate_fn,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 4000\n",
      "Validation set length: 1000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the lengths of the training and validation sets\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(game_size=1, dataset=train_data, batch_size=50, shuffle=True)\n",
    "val_loader = DataLoader(game_size=1, dataset=val_data, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs for batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[200, 2], edge_index=[2, 500], edge_attr=[500, 3], target_node_idx=[50], root_idx=[50], batch=[200], ptr=[51])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m graphs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mto_data_list()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m graph \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(graphs)):\n\u001b[0;32m----> 3\u001b[0m     sequence_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_graph_to_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sequence_tensor)\n",
      "Cell \u001b[0;32mIn[17], line 67\u001b[0m, in \u001b[0;36mprocess_graph_to_sequence\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([sequence], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Process graph to sequence\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m node_info \u001b[38;5;241m=\u001b[39m \u001b[43mget_node_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m tree \u001b[38;5;241m=\u001b[39m sort_tree(node_info, graph\u001b[38;5;241m.\u001b[39mtarget_node_idx)\n\u001b[1;32m     69\u001b[0m sequence_tensor \u001b[38;5;241m=\u001b[39m build_sequence(tree, node_info, vocab)\n",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m, in \u001b[0;36mprocess_graph_to_sequence.<locals>.get_node_info\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m      7\u001b[0m node_info \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      8\u001b[0m relationship_types \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;241m1.\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m]): \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarried-to\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m1.\u001b[39m, \u001b[38;5;241m0.\u001b[39m]): \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchild-of\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m1.\u001b[39m]): \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgave-birth-to\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, features_tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m):\n\u001b[1;32m     11\u001b[0m     features \u001b[38;5;241m=\u001b[39m features_tensor\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     12\u001b[0m     gender \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmale\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfemale\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "graphs = batch[3].to_data_list()\n",
    "for graph in range(0, len(graphs)):\n",
    "    sequence_tensor = process_graph_to_sequence(graph)\n",
    "    print(sequence_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[200, 2], edge_index=[2, 500], edge_attr=[500, 3], target_node_idx=[50], root_idx=[50], batch=[200], ptr=[51])\n",
      "tensor([3, 1, 0, 2, 0, 1, 3, 3, 0, 1, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 1, 3, 3, 0,\n",
      "        2, 3, 2, 1, 3, 0, 1, 2, 2, 1, 0, 1, 0, 1, 2, 2, 2, 3, 2, 2, 0, 0, 2, 2,\n",
      "        0, 1])\n",
      "Sender's shape:  torch.Size([50, 20])\n",
      "Receiver's shape:  torch.Size([50, 4])\n"
     ]
    }
   ],
   "source": [
    "print(batch[3])\n",
    "print(batch[3].target_node_idx)\n",
    "\n",
    "# Sender produces a message\n",
    "sender_output = sender(None, batch[3])\n",
    "# print(\"Sender's message:\", sender_output)\n",
    "print(\"Sender's shape: \", sender_output.shape)\n",
    "\n",
    "# Receiver tries to identify the target node\n",
    "receiver_output = receiver(sender_output, None, batch[3])\n",
    "# print(\"Receiver's output:\", receiver_output)\n",
    "print(\"Receiver's shape: \", receiver_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sender's shape: torch.Size([50, 5, 100])\n",
      "Receiver's output shape: torch.Size([50, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "sender_gs = core.RnnSenderGS(sender, opts.vocab_size, opts.embedding_size, opts.hidden_size, max_len=opts.max_len, temperature=opts.gs_tau, cell=opts.sender_cell)\n",
    "receiver_gs = core.RnnReceiverGS(receiver, opts.vocab_size, opts.embedding_size, opts.hidden_size, cell=opts.sender_cell)\n",
    "\n",
    "# Sender produces a message\n",
    "sender_output = sender_gs(None, batch[3])\n",
    "# print(\"Sender's message:\", sender_output)\n",
    "print(\"Sender's shape:\", sender_output.shape) # batch size x max_len+1 x vocab size\n",
    "\n",
    "# Receiver tries to identify the target node\n",
    "receiver_output = receiver_gs(sender_output, None, batch[3])\n",
    "# print(\"Receiver's output:\", receiver_output)\n",
    "print(\"Receiver's output shape:\", receiver_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_nll(\n",
    "    _sender_input, _message, _receiver_input, receiver_output, labels, _aux_input):\n",
    "    \"\"\"\n",
    "    NLL loss - differentiable and can be used with both GS and Reinforce\n",
    "    \"\"\"\n",
    "    # print(receiver_output)\n",
    "    # print(labels)\n",
    "    nll = F.nll_loss(receiver_output, labels, reduction=\"none\")\n",
    "    acc = (labels == receiver_output.argmax(dim=1)).float().mean()\n",
    "    return nll, {\"acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400])\n"
     ]
    }
   ],
   "source": [
    "print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 1.3164544105529785, \"acc\": 0.41999974846839905, \"length\": 4.999938488006592, \"mode\": \"train\", \"epoch\": 1}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3160 and the array at index 1 has size 7998000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 26\u001b[0m\n\u001b[1;32m     10\u001b[0m topographic_similarity \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mTopographicSimilarity(\n\u001b[1;32m     11\u001b[0m     sender_input_distance_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     12\u001b[0m     message_distance_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     is_gumbel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     19\u001b[0m     game\u001b[38;5;241m=\u001b[39mgame, \n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[core\u001b[38;5;241m.\u001b[39mConsoleLogger(as_json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, print_train_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), topographic_similarity]\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/trainers.py:276\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    273\u001b[0m train_loss, train_interaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_epoch()\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 276\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_interaction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m validation_loss \u001b[38;5;241m=\u001b[39m validation_interaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_freq \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    283\u001b[0m ):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/language_analysis.py:160\u001b[0m, in \u001b[0;36mTopographicSimilarity.on_epoch_end\u001b[0;34m(self, loss, logs, epoch)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss: \u001b[38;5;28mfloat\u001b[39m, logs: Interaction, epoch: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_topsim_train_set:\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/language_analysis.py:210\u001b[0m, in \u001b[0;36mTopographicSimilarity.print_message\u001b[0;34m(self, logs, mode, epoch)\u001b[0m\n\u001b[1;32m    207\u001b[0m messages \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    208\u001b[0m sender_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(logs\u001b[38;5;241m.\u001b[39msender_input, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 210\u001b[0m topsim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_topsim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msender_input_distance_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage_distance_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m output \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mdict\u001b[39m(topsim\u001b[38;5;241m=\u001b[39mtopsim, mode\u001b[38;5;241m=\u001b[39mmode, epoch\u001b[38;5;241m=\u001b[39mepoch))\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m#print(output, flush=True) # Modified\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/egg/core/language_analysis.py:201\u001b[0m, in \u001b[0;36mTopographicSimilarity.compute_topsim\u001b[0;34m(meanings, messages, meaning_distance_fn, message_distance_fn)\u001b[0m\n\u001b[1;32m    198\u001b[0m meaning_dist \u001b[38;5;241m=\u001b[39m distance\u001b[38;5;241m.\u001b[39mpdist(meanings, meaning_distance_fn)\n\u001b[1;32m    199\u001b[0m message_dist \u001b[38;5;241m=\u001b[39m distance\u001b[38;5;241m.\u001b[39mpdist(messages, message_distance_fn)\n\u001b[0;32m--> 201\u001b[0m topsim \u001b[38;5;241m=\u001b[39m \u001b[43mspearmanr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeaning_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraise\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcorrelation\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m topsim\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/scipy/stats/_stats_py.py:5427\u001b[0m, in \u001b[0;36mspearmanr\u001b[0;34m(a, b, axis, nan_policy, alternative)\u001b[0m\n\u001b[1;32m   5425\u001b[0m b, _ \u001b[38;5;241m=\u001b[39m _chk_asarray(b, axis)\n\u001b[1;32m   5426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axisout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 5427\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5428\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5429\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrow_stack((a, b))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/numpy/lib/shape_base.py:652\u001b[0m, in \u001b[0;36mcolumn_stack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    650\u001b[0m         arr \u001b[38;5;241m=\u001b[39m array(arr, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ndmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    651\u001b[0m     arrays\u001b[38;5;241m.\u001b[39mappend(arr)\n\u001b[0;32m--> 652\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3160 and the array at index 1 has size 7998000"
     ]
    }
   ],
   "source": [
    "game = core.SenderReceiverRnnGS(sender_gs, receiver_gs, loss_nll)\n",
    "\n",
    "core.init(params=['--random_seed=7', \n",
    "                '--lr=1e-2',   \n",
    "                f'--batch_size={opts.batch_size}',\n",
    "                '--optimizer=adam'])\n",
    "\n",
    "optimizer = torch.optim.Adam(game.parameters())\n",
    "\n",
    "topographic_similarity = core.TopographicSimilarity(\n",
    "    sender_input_distance_fn=\"cosine\", \n",
    "    message_distance_fn=\"euclidean\", \n",
    "    compute_topsim_train_set=True, \n",
    "    compute_topsim_test_set=True, \n",
    "    is_gumbel=True\n",
    ")\n",
    "\n",
    "trainer = core.Trainer(\n",
    "    game=game, \n",
    "    optimizer=optimizer, \n",
    "    train_data=train_loader,\n",
    "    validation_data=val_loader, \n",
    "    callbacks=[core.ConsoleLogger(as_json=True, print_train_loss=True), topographic_similarity]\n",
    ")\n",
    "\n",
    "trainer.train(n_epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
